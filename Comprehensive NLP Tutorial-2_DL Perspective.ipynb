{"cells":[{"metadata":{},"cell_type":"markdown","source":"This tutorial is part-2 of [Comprehensive tutorial on NLP](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-1-ml-perspective) series. In this part we will learn about word embedding and see how Deep learning has simplified NLP processing. \n\nPre-requisite: Basic Deep learning understanding would be helpful though not mandatory."},{"metadata":{},"cell_type":"markdown","source":"<a class=\"kk\" id=\"0.1\"></a>\n## Contents\n\n1. [Introduction to Word Embedding](#1)\n    1. [Dimensionality](#1.1) \n    1. [Padding](#1.2)\n    1. [Euclidean Distance](#1.3)\n    1. [Cosine Similarity](#1.4)\n1. [Word Embedding Techniques](#2)\n    1. [Word2Vec](#2.1)\n        1. [Skip-Gram](#2.1.1)\n        1. [CBOW (Continuous Bag of Words)](#2.1.2)\n    1. [GloVe](#2.2) \n    1. [FastText](#2.3)\n1. [Text to Numeric Convertion Using Word Vectors](#3)\n    1. [Vector Averaging](#3.1)\n        1. [Vector Averaging With Word2Vec](#3.1.1)\n        1. [Vector Averaging With GloVe](#3.1.2)\n        1. [Vector Averaging With FastText](#3.1.3) \n    1. [Embedding Matrix & Keras Embedding layer](#3.2)\n        1. [Word2Vec Embedding layers](#3.2.1)\n        1. [GloVe Embedding layers](#3.2.2)\n        1. [FastText Embedding layers](#3.2.3)      \n1. [Deep Learning models](#4)\n    1. [Basic-DNN](#4.1)\n    1. [CNN](#4.2)\n    1. [RNN](#4.3)\n    1. [Recurrent Neural Network -LSTM](#4.4)\n    1. [Recurrent Neural Network – GRU](#4.5)\n "},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction to Word Embedding  <a class=\"kk\" id=\"1\"></a>\n[Back to Contents](#0.1)\n\nWord Embedding is also known as Word Vectorization. It means converting word into vector. Vectors are numeric representation of a point in space. Mathematically vectors are 1D array or sequence of numbers.  \n\n\n<B>Why we need Word Embedding? </B>\n\nA problem with our previous text to numeric conversion techniques was that they ignore synonyms for example word 'measure' and ‘calculate’ were represented differently however in most sentences they can be used interchangeably. In Word Embedding similar words are spatially close to each other in vector space. Word Embedding is also capable of preserving semantic and syntactic similarity and relation with other words. The vector representation are such that geometric transformation adopts syntax and semantic. For instance, by adding a “female” vector to the vector “king”, we obtain the vector “queen” and by adding a “plural” vector to the vector “king”, we obtain “kings”. \n\nAnother problem we observe in part 1 was production of high dimensionality sparse matrix. Word Embedding produces low dimensionality dense matrix.\n\n<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1088431/textproc.jpg?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587453060&Signature=iqgM8wHVoGLZpk3pPqy7WgM5FSj6vCKTwFLgTJMcFcvTepG4SeTUJS8OMi%2BY%2BcrbeahiyWpvnWIkOF%2FoK%2FjLxLdW5fYN2%2BU25YnuQ7mNSzRWeq60rMbCKmBRPvPsTt4dp9XxtUv2spMYh6%2FTHM0nAlRpd0t5oJ0vgFly%2BjXNw0tVlb80cCfOPWLFbhnjHl8iQ0tPWzcnXY0ggE2UPGE14t7jHp%2FNCTid6hy7xBAXHDIHYD7u54UsG9Jg2F0Wzo54aKaeALyrRCkgCrNPHDYQHeBOD2kuUw8hxHkQW4qTDveVo4wD7wSsm%2BDU4dgimbNif3GnxFUvISA%2BSOGW9gGQfw%3D%3D\" width=\"250\">\n\nBefore applying Word Embedding techniques lets look into into few common NLP vocabulary terms.\n "},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Dimensionality  <a class=\"kk\" id=\"1.1\"></a>\n\nDimensionality refers to the length of vectors."},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Padding <a class=\"kk\" id=\"1.2\"></a>\nPadding is task of appending string up to given specific length with whitespaces. Padding is used to represent all records as fixed length."},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Euclidean Distance <a class=\"kk\" id=\"1.3\"></a>\n\nEuclidean distance is the shortest distance between two points in (Euclidean) space.\n\n<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1088431/eucldeandistance.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587453143&Signature=XnIX7CD9Lz9rVlZBTu6obdj3x5etPzJTCqbK3aGxDmHRRGUITtPpgKOdR6A37femrh24tdeLrXZ3NNDufdYmef2bqBHLCSop93DFA5l54ytuwvA4ve0YsfhiFEJcZn4%2BjpMtZGAIMA4aJuRdmhVZFLNQvMd41DV1%2FavIugMelQJ7iP0fg%2BkUwD0NuAQYvCjT6H%2FEyeTLbCLPPK2QZtlUReFxvoVy7UcMPzrlVtfPLODSTTjSStOPQ6Yj4mT2zZjqa5q22HhlP43pPIZCjpetQs10uySqn1z5ye25wUPC74fgglUhlhw%2BYXnsSkFK2Y3C0hNLIgb6aQ%2FZKi0JQT24Dw%3D%3D\" width=\"250\">"},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Cosine Similarity  <a class=\"kk\" id=\"1.4\"></a>\nCosine similarity is a measure of similarity between two nonzero vectors of an inner product space. It measures the cosine of the angle between them.\n\n<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1088431/cosineSimilarity.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587453176&Signature=AJGrzSlx5UZQh3Gyp5B6P4zaEVcT7PaFyBeeDqg5plJ%2B1rjdEdmGVKdvN2wtpd0Fx2qPBUkZK5doqzyh6mRi%2BOWP5v0NVgg2Bur0SlzZYmEV8SnBFbwvPNrJXPXTTiFFeNg8hTN%2FiL2GP1FS8GRKSlKSJlVjUZiM8EKqcwEJpPCbIZ5l%2FWlrf44e8iH9O096%2BuiqZk%2B8wsOYZd4jta%2Fy%2F4bJSnL837GOMXkhxI7IsCsPfcZTKPORG0ucxql1kTMschsR%2BHKRTJZubcEeSHbMbS7nrj2t1OziDishlQm2bSpH4UzparxXJ6OOk5GHE4t66IOnda0GSxVreNd9AHGklA%3D%3D\" width=\"250\">\n "},{"metadata":{},"cell_type":"markdown","source":"# 2. Word Embedding Techniques <a class=\"kk\" id=\"2\"></a>\n[Back to Contents](#0.1)\n\nNow we will look into word Embedding techniques but before that let's fetch our [Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) dataset and clean it as we did in part 1."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Fetch & clean dataset \n\n# !pip install pyspellchecker\n# from spellchecker import SpellChecker\nimport pandas as pd\nfrom nltk.corpus import stopwords \nfrom nltk.corpus import wordnet\n\nfrom nltk.stem import WordNetLemmatizer \nimport nltk \nimport re\nimport numpy as np  \nimport pandas as pd \ntrain_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n\n\ndef convert_to_antonym(sentence):\n    words = nltk.word_tokenize(sentence)\n    new_words = []\n    temp_word = ''\n    for word in words:\n        antonyms = []\n        if word == 'not':\n            temp_word = 'not_'\n        elif temp_word == 'not_':\n            for syn in wordnet.synsets(word):\n                for s in syn.lemmas():\n                    for a in s.antonyms():\n                        antonyms.append(a.name())\n            if len(antonyms) >= 1:\n                word = antonyms[0]\n            else:\n                word = temp_word + word # when antonym is not found, it will\n                                    # remain not_happy\n            \n            temp_word = ''\n        if word != 'not':\n            new_words.append(word)\n    return ' '.join(new_words)\n\n\n# def correct_spellings(text):\n#     spell = SpellChecker()\n#     corrected_words = []\n#     misspelled_words = spell.unknown(text.split())\n#     for word in text.split():\n#         if word in misspelled_words:\n#             corrected_words.append(spell.correction(word))\n#         else:\n#             corrected_words.append(word)\n#     return \" \".join(corrected_words)\n\n        \nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV }\ndef lemma_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word ,wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n \ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n  \"\"\"\n    text = text.lower() # lowercase text\n    text= re.sub(r'[^\\w\\s#]',' ',text) #Removing every thing other than space, word and hash\n    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n    text= re.sub(r'[0-9]',' ',text)\n    #text = correct_spellings(text)\n    text = lemma_words(text)\n    text = convert_to_antonym(text)\n    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    \n    return text\n\n\ntrain_df['text'] = train_df['text'].apply(clean_text)\ntest_df['text'] = test_df['text'].apply(clean_text)\n\nsentences= pd.DataFrame(columns=['text'])\nsentences['text']= pd.concat([train_df[\"text\"], test_df[\"text\"]])\n\nfrom collections import defaultdict\ntokens_list = [row.split() for row in sentences['text']]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Word2Vec    <a class=\"kk\" id=\"2.1\"></a>\n\n Word2Vec is group of related models that are used to produce Word Embeddings. It was created & patented by Tomas Mikolov and a group of a research team from Google in 2013. Each unique word in the corpus is assigned a corresponding vector in the space. Word2Vec relies only on local information of language hence the semantics learnt for a given word is only affected by the surrounding words. Underlying assumption of Word2Vec is that two words sharing similar contexts also share a similar meaning, this at times results into similar vector representation (cosine similarity) of multiple words.One more drawback of word2vec is its unablity to takecare of OOV word. \n\n <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1088431/word2vec.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587453288&Signature=N0%2F8z5K282OSq7Yr7xB5xzrCXx8zqZbZDhOfRwdGyr7%2Bu9rOjQkOQgW7bNZ9XYHDnrDCcGI2zp57OwTBEUF8DLjncjTGH7sykg4eaeCJ9LhGITFzf%2FbLVHPk4cVkVQ9iJUHWSnYXxy8bpxuz4KstbCCnn0eyihRjqe50eiTIYv90V%2FaSVMHQQmLlMxi8EDQxF6gIgqgiwMr%2BwhIxNkxA9ApXpo3G5XAhGDJGQq4amgyAFaAlHr5ueqAv%2Bc77wDCoEAbCpLpxrFRRqqpdSBuE8cIqV6rC02WsgpZk3FvnRbw%2FePIKSdVXWlHv5fkPXInFV8Do48fhtzF2hHbrPEl8Og%3D%3D\" width=\"250\">\n \n we will see error when we try to buid  Word2Vec comes in two flavours,\n - Skip-Gram and \n - Continuous Bag of Words (CBOW)\n\nUnderneath Word2Vec uses neural network algorithms that can be trained on any type of sequential data. Fortunately we have libraries available that have already implemented these algorithms and we have to just call the method with proper arguments. A popular one among such libraries is <B>gensim</B>. It provides the [Word2Vec Class](https://radimrehurek.com/gensim/models/word2vec.htm) for working with a Word2Vec model.\n"},{"metadata":{},"cell_type":"markdown","source":"##### Gensim implementation of Word2vec\n\n\n<B>Arguments:</B>\n\n- min_count : Minimum number of occurrences of a word in the corpus to be included in the model. The higher the number, the less words we have in our corpus\n- window: The maximum distance between the current and predicted word within a sentence\n- size: The dimensionality of the feature vectors\n- workers: no of cores\n- sg = 1  for skipgram and 0 for cbow\n\n- sample = (type float) - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n\n- alpha = float - The initial learning rate - (0.01, 0.05)\n\n- min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n\n- negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n\n\n<B>Function Calls :</B> \n- model.build_vocab: Prepare the model vocabulary\n- model.train: Train word vectors\n- model.init_sims(): When we do not plan to train the model any further, we use this line of code to make the model more memory-efficient\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"###  2.1.1 Skip-Gram  <a class=\"kk\" id=\"2.1.1\"></a>\n \nSkip-Gram is designed to predict the context from base word. From a given word, Skip-gram model tries to predict its neighbouring words.  <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1088431/skipgram.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587453316&Signature=Ffma5YpXMX0beUVshSavBGbxq%2Fas3VPpIcvkiLTZJcqz1MyUEI5DtPsSQnOpysFo7%2BjhB47HgVuMYw5vuXF3MzTyyCZdaY49%2FN4IhNfiqRF7B4qtZb4mVUG0eETZpzzlX2WssPfDsfLSxB52TWK3dGOJ%2Bukq19aGO2XndlioGwRfyK7DVqNbHr8DRJ5Y4j7XP1Cd5XByZWf0zqdrcFyjRvVnVgyxCMTQmjCWxvfMTANXS%2FhxhfUXhmGbJxBpGuyb9EXOhKRLzo2Py42%2B%2F2X9SpChM7kKmtxvDH8ejZsqRuGHxDWqnYT%2BbAuP0hr95QqEYTTfx8DzwQCXvSxcabKTgQ%3D%3D\" width=\"250\">\n\n Skip-gram is a [(target, context), relevancy] generator. Skip-gram generator gives us pair of words and their relevance (a float value). Lets generate Word2Vec skip-gram embedding for our cleaned-up text dataset using gensim.  "},{"metadata":{},"cell_type":"markdown","source":"##### Building Skipgram  WordVectors using gensim"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom time import time\nt = time()\n# initialize skipgram model\nsg_model = Word2Vec(min_count=2,window=2,size=300, sg = 1,sample=5e-5, alpha=0.05, min_alpha=0.0005,negative=20 )\n# build model vocabulary\nsg_model.build_vocab(tokens_list)\n\n# train the model\nsg_model.train(tokens_list, total_examples=sg_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to build Skip gram model vocab: {} mins'.format(round((time() - t) / 60, 2)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have just build our first word-embedding model.and that also with only 3 lines of code. Lets play with the model"},{"metadata":{},"cell_type":"markdown","source":"##### vector representation of word"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"sg_model.wv.__getitem__('hope')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Validate dimension of our word vector"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"len(sg_model.wv.__getitem__('hope'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Measure similarity   b/w two words "},{"metadata":{"scrolled":false,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"sg_model.wv.similarity('people','saint' )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sg_model.wv.similarity('people', 'terrorist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Fetch most similar words  wrt any given word "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"sg_model.wv.most_similar('fire')[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Fetch list of word vocabulary "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"print(list(sg_model.wv.vocab))\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.2 CBOW (Continuous Bag of Words)  <a class=\"kk\" id=\"2.1.2\"></a>\n\nCBOW is designed to predict the base(target) word from context. CBOW is faster to train than the skip-gram and gives slightly better accuracy for the frequent words.\n\n<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1088431/cbow.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587453363&Signature=GcHf9k9J%2Fb9fUj0uLQnskT1eIvc9KKk0RGkQyCjj3jrKh9QBfsoDJtKuRMfZQtGAMoax%2B%2Bmv7lYhtXBwS3wi0D8EqPwl%2FFY%2FZdb4XXh2W4pFlgpDmUofgS6MwfMbSoa2NbIM%2FwByhATzTCb%2BDRKk9ZAcXqgg6%2BnYLfwD3VdKq9DUzUqumQK%2BEpUZaQVYJGL6mFwm9sDIORwaMPmh0P1RtAGe4824LyChRP70JBFke3sVms2gj8P4I6UmlOfsgmpm2h0rBYSAr3I8BmTvpiaEy08LZ4ge6W5ApagaIxSTXm1axYzWUGS3A91ycvYOk%2Bur1VKUyKgWItk0mdeat7s61g%3D%3D\" width=\"250\">\n\n"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#### Building CBOW wordvectors\nfrom gensim.models import Word2Vec\nfrom time import time\nt = time()\n# initialize\ncbow_model = Word2Vec(min_count=2,window=2,size=300, sg = 0,sample=5e-5, alpha=0.05, min_alpha=0.0005, \n                     negative=20 )\n# build model vocabulary\ncbow_model.build_vocab(tokens_list)\n\n# train the model\ncbow_model.train(tokens_list, total_examples=cbow_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to build CBOW model vocab: {} mins'.format(round((time() - t) / 60, 2)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Did you notice CBOW trained faster ?\n"},{"metadata":{},"cell_type":"markdown","source":"####  Pretrain Word2Vec\n\nGoogle has made available pretrained word embedding which includes word vectors for a vocabulary of 3 million words and phrases that they have trained on roughly 100 billion words from Google News dataset using Word2Vec."},{"metadata":{"trusted":true},"cell_type":"code","source":"#fetching  pretrain wordvector\nfrom gensim.models.keyedvectors import KeyedVectors\nt = time()\npretrained_w2vec_embedding = KeyedVectors.load_word2vec_format('../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin', binary=True)\nprint('Time to fetch  pretrain  Word2Vec model vocab: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### vector representation of Word2Vec pretrained word "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"pretrained_w2vec_embedding['people']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# shape of pretrain w2vec embedding\npretrained_w2vec_embedding.vectors.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 GloVe  <a class=\"kk\" id=\"2.2\"></a>\n\n[GloVe](https://nlp.stanford.edu/pubs/glove.pdf) stands for \"Global Vectors\". It is a Word Embedding [project](https://nlp.stanford.edu/projects/glove/)  written in C language and developed by Stanford university researchers in 2014. Glove embedding technique is based on (first) construction of a co-occurrence matrix from a training corpus and then (second) factorization of co-occurrence matrix in order to yield word vector.\n\nUnlike word2vec which captures only local statistics of token Glove captures both global statistics and local statistics of a text tokens. Its embeddings relate to the probabilities that two words appear together. [glove_python](https://github.com/maciejkula/glove-python) library provides glove implementation."},{"metadata":{},"cell_type":"markdown","source":"#####  Implementation of Glove via  glove_python\n\n\nCall-Arguments description : \n\n1. For corpus.fit()  :\n    - lines : this is the 2D array we created after the pre-processing\n    - window : this is the distance between two words algorithm should consider to find some relationship between them\n    \n    \n2. For glove() :\n    - no_of_components : This is the dimension of the output vector generated by the GloVe\n    - learning_rate : Algo uses gradient descent so learning rate defines the rate at which the algo reaches towards the minima (lower the rate more time it takes to learn but reaches the minimum value)\n\n\n3. For glove.fit() :\n    - co occurrence matrix: the matrix of word co-occurrences\n    - epochs: this defines the number of passes algo makes through the data set\n    - no_of_threads: number of threads used by the algo to run"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"!pip install glove_python","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"#importing the glove library\nfrom glove import Corpus, Glove\n\n# creating a corpus object\ncorpus = Corpus() \n\n#training the corpus to generate the co occurence matrix which is used in GloVe\ncorpus.fit(tokens_list, window=3)\n\n#creating a Glove object which will use the matrix created in the above lines to create embeddings\n#we can set the learning rate as glove uses Gradient Descent\nglove = Glove(no_components=300, learning_rate=0.05)\nglove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\nglove.add_dictionary(corpus.dictionary)\nglove.save('glove.model')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  Displaying Glove WordVector of a word "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"glove.word_vectors[glove.dictionary['people']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Pretrain Glove\n\nGlove developers have also made available pre-computed embeddings for millions of English tokens, obtained from training Wikipedia data and Common crawl data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fetch pretrain glove word vectors \nimport numpy as np \npretrained_glove_embedding={}\nwith open('../input/nlpword2vecembeddingspretrained/glove.6B.300d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        pretrained_glove_embedding[word]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####   Glove pretrained WordVector of a word "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"pretrained_glove_embedding['hello']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no of  word in pretrained glove embedding\nlen(pretrained_glove_embedding)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3. Fast-Text <a class=\"kk\" id=\"2.3\"></a>\n\n[FastText](https://fasttext.cc/) is a library for learning of word embeddings and text classification. The Facebook Research Team created fastText in Nov 2015. Fast-Text is an extension of word2vec library. It builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. FastText assumes a word to be formed by a n-grams of character for example, sunny is composed of [sun, sunn,sunny],[sunny,unny,nny]... etc, where n could range from 1 to the length of the word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. Thus even for previously unseen words, typo errors, and OOV (Out Of Vocabulary) words the model can make an educated guess towards its meaning.Obvious trade off is processing time. Gensim provides the [FastText implementation](https://radimrehurek.com/gensim/models/fasttext.html).\n\n"},{"metadata":{},"cell_type":"markdown","source":"##### FastText Implementatation using gensim\n\nRefer : https://radimrehurek.com/gensim/models/fasttext.html for parameter details"},{"metadata":{"trusted":true},"cell_type":"code","source":"dimension =300\nfrom gensim.models import FastText\nfasttext_model = FastText(tokens_list, size=dimension, window=5, min_count=5, workers=4, sg=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  Displaying FastText WordVector of given word "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"fasttext_model.wv.__getitem__('people')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# fasttext word similarity measure\nfasttext_model.wv.similarity('evacuation','shelter' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# most similar words\nfasttext_model.wv.most_similar('fire')[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### PreTrained Fasttext\n\nFastText developers have also made available pre-computed embeddings for millions of english tokens, obtained from training Wikipedia data and common crawl data. \n\nDisclaimer: Loading the fastText pretrain will consume some serious memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\n\nEMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\npretrained_fasttext_embedding = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in (open(EMBEDDING_FILE)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# no of words in pretrained fasttext embedding\nlen(pretrained_fasttext_embedding)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  Displaying FastText pretrained WordVector of a word "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"pretrained_fasttext_embedding['fire']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"len(pretrained_fasttext_embedding['fire'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del pretrained_fasttext_embedding\ndel fasttext_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Text to Numeric Convertion Using Word Vectors <a class=\"kk\" id=\"3\"></a>\n[Back to Contents](#0.1)\n\nSo we have learned about word embedding techniques and created word vectors for our corpus.  Now we will convert our textual data into numerical using these word vectors.  I will explain about two popular texts to numerical conversion techniques using word vectors, \n1. Vector Averaging  \n2. Embedding Matrix and Keras Embedding layer\n"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Vector Averaging  <a class=\"kk\" id=\"3.1\"></a>\nIn this approach we directly averages all word embedding occurred in the text. Final length remains equal to word vector dimension. This is go to technique when we are planning to use standard machine learning models such a logistic regression, naïve-bayes, svm etc.  \n "},{"metadata":{},"cell_type":"markdown","source":"### Vector Averaging With Word2Vec <a class=\"kk\" id=\"3.1.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# functions for Vector Averaging with word2Vec\nimport numpy as np\ndef w2v_embeddings(text,w2v_model,dimension):\n    if len(text) < 1:\n        return np.zeros(dimension)\n    else:\n        vectorized = [w2v_model.wv[word] if word in w2v_model.wv else np.random.rand(dimension) for word in text] \n    \n    sum = np.sum(vectorized,axis=0)\n    ## return the average\n    return sum/len(vectorized)     \n\ndef get_w2v_embeddings(text,w2v_model,dimension):\n        embeddings = text.apply(lambda x: w2v_embeddings(x, w2v_model,dimension))\n        return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SkipGram Model"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Text to Numeric Vector Averaging using sgmodel \ntrain_embeddings_sg_model  = get_w2v_embeddings(train_df['text'],sg_model,dimension=300)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validate train set size\nlen(train_embeddings_sg_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# validate dimension\nlen(train_embeddings_sg_model[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### CBOW model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text to numeric Vector Averaging using cbow model\ntrain_embeddings_cbow_model_  = get_w2v_embeddings(train_df['text'],cbow_model,dimension=300)\n \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vector Averaging With Glove <a class=\"kk\" id=\"3.1.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# functions  for Vector Averaging with GloVe\nimport numpy as np\ndef glove_embeddings(text, glove_model, dim ):\n    dic=glove_model.dictionary\n    if len(text) < 1:\n        return np.zeros(dim)\n    else:\n        vectorized = [glove_model.word_vectors[dic[word]] if word in dic else np.random.rand(dim) for word in text]  \n    sum = np.sum(vectorized,axis=0)\n    ## return the average\n    return sum/len(vectorized)     \n\ndef get_glove_embeddings(text,glove_model,dimension):\n        embeddings = text.apply(lambda x: glove_embeddings(x,glove_model, dimension))\n        return list(embeddings)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text to numeric Vector Averaging using glove\nimport numpy as np\ntrain_embeddings_glove = get_glove_embeddings(train_df['text'],glove,dimension=300)\ntest_embeddings_glove = get_glove_embeddings(test_df['text'],glove,dimension=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vector Averaging With Fasttext  <a class=\"kk\" id=\"3.1.3\"></a>\n\nAs Fastext is an extension of word2vec hence the same averaging function of w2vec i.e.`get_ w2v_embeddings` will work for fasttext model too. "},{"metadata":{"trusted":true},"cell_type":"code","source":"###  Text to numeric using Averaging with Fasttext\nimport numpy as np\nfasttext_train_embeddings = w2v_embeddings(train_df['text'], fasttext_model,dimension=300)\nfasttext_test_embeddings = w2v_embeddings(test_df['text'],  fasttext_model,dimension=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Vector Averaging With pretrained Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def pretrained_embeddings(text,model,dimension):\n#     if len(text) < 1:\n#         return np.zeros(dimension)\n#     else:\n#         vectorized = [model[word] if word in model else np.random.rand(dimension) for word in text] \n    \n#     sum = np.sum(vectorized,axis=0)\n#     ## return the average\n#     return sum/len(vectorized)     \n\n# def get_pretrained_embeddings(text,model,dimension):\n#         embeddings = text.apply(lambda x: pretrained_embeddings(x, model,dimension))\n#         return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text to numeric Vector Averaging  using  pretrianed word2vec\n# train_embeddings_w2vec_pretrained_  = get_pretrained_embeddings(train_df['text'],pretrained_w2vec_embedding,dimension=300)\n\n# Text to numeric Vector Averaging  using  pretrianed glove\n# train_embeddings_glove_pretrained_  = get_pretrained_embeddings(train_df['text'],pretrained_glove_embedding,dimension=300)\n\n# Text to numeric Vector Averaging  using  pretrianed fasttext\n# train_embeddings_fasttext_pretrained_  = get_pretrained_embeddings(train_df['text'],pretrained_fasttext_embedding,dimension=300)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Embedded Matrix & Keras Embedding layer <a class=\"kk\" id=\"3.2\"></a>\n\nAveraging is preferred choice when we intend to use ML models such as lr, svm, gbm etc. but our purpose here is to utilise Deep-learning algorithms. Deep Learning is a layer bases learning where each layer passes its learning to the next layer.   Few libraries have implement deep learning algorithms. A popular one among them is Keras. We will use Keras for our deep learning modelling purpose.\n\nFor text processing Keras offers an embedding layer. This is the first layer of deep learning algorithm. Weights of the Embedding layer are of the shape (vocabulary_size, embedding_dimension), this weight matrix is also called as Embedding matrix. We will first generate this embedding matrix from our word vectors and then initialize Keras embedding layer for each of our word embeddings. \n\nMoreover, Keras has built-in utilities for doing tokenization and encoding of text. We will use these utilities as they take care of a number of important features such as stripping special characters from strings, padding, fetching N most common words in dataset etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# using keras built in utilities\n# tokenizing using keras  tokenizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.layers import Embedding\ntokenizer_obj=Tokenizer()\n# to builds the word index\ntokenizer_obj.fit_on_texts(tokens_list)\n# to turns strings into lists of integer indices.\nsequences=tokenizer_obj.texts_to_sequences(tokens_list)\n# defining maximum length of sequence \nMAX_LEN= 50\n# pad_sequences is used to ensure that all sequences in a list have the same length\ntweet_pad= pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n\n# segregating text & train from corpu\nx_train = tweet_pad[:7613]\nx_test = tweet_pad[7613:]\n\ntargets =  [target for target in train_df['target']]\n\n# set of all word and their sequence no\nword_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))\nvocab_size = len(word_index)+1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to generate embeeding layer weights i.e. embeeding_matrix\n\n\ndef generate_word2vec_embeeding_matrix(word_vector_model, dimension, vocab_size= vocab_size, word_index =word_index):\n    embedding_matrix=np.zeros((vocab_size,dimension))\n    for word,i in tqdm(word_index.items()):\n        if i > vocab_size:\n            continue\n        if word in word_vector_model.wv:  \n            emb_vec=word_vector_model.wv.__getitem__(word)\n            embedding_matrix[i]=emb_vec\n    return embedding_matrix\n\ndef generate_pretrained_embeeding_matrix(word_vector_model, dimension, vocab_size= vocab_size, word_index =word_index):\n    embedding_matrix=np.zeros((vocab_size,dimension))\n    for word,i in tqdm(word_index.items()):\n        if i > vocab_size:\n            continue\n        if word in word_vector_model:  \n            emb_vec=word_vector_model[word]\n            embedding_matrix[i]=emb_vec\n    return embedding_matrix\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create Keras embedding layers for our word embeddings. As we have created 4 trained word embedding model (skipgram, cbow, glove and fasttext) and 3 pretrained model (one each for word2vec, glove and fasttext) for all these seven we will create a Keras embedding layer.\n\n### Word2Vec Embedding layers  <a class=\"kk\" id=\"3.2.1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"##### Trained skipgram"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from keras.layers import Embedding\n\nembedding_matrix_sg_trained = generate_word2vec_embeeding_matrix(sg_model, dimension = 300)\n\nembedding_layer_sg_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_sg_trained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Pre-Trained  Word2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pre trainde word2vec  \nembedding_matrix_w2v_pretrained = generate_pretrained_embeeding_matrix(pretrained_w2vec_embedding, dimension =300)    \n\nembedding_layer_w2v_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_w2v_pretrained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Trained  CBOW"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix_cbow_trained = generate_word2vec_embeeding_matrix(cbow_model, dimension = 300)\n\nembedding_layer_cbow_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_cbow_trained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GloVe Embedding Layers   <a class=\"kk\" id=\"3.2.2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"##### Trained Glove"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nembedding_matrix_glove_trained=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    if i > vocab_size:\n        continue\n    \n    emb_vec=glove.word_vectors[glove.dictionary[word]]\n    if emb_vec is not None:\n        embedding_matrix_glove_trained[i]=emb_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer_glove_trained = Embedding(vocab_size, 300 , weights=[embedding_matrix_glove_trained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### PreTrained Glove"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix_glove_pretrained = generate_pretrained_embeeding_matrix(pretrained_glove_embedding, dimension =300)    \n\nembedding_layer_glove_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_glove_pretrained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fasttext  Embedding layers  <a class=\"kk\" id=\"3.2.3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"##### Trained FastText"},{"metadata":{"trusted":true},"cell_type":"code","source":" \nembedding_matrix_fasttext_trained = generate_word2vec_embeeding_matrix(fasttext_model, dimension =300)    \n\nembedding_layer_fasttext_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_fasttext_trained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Pre-Trained FastText"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix_fasttext_pretrained = generate_pretrained_embeeding_matrix(pretrained_fasttext_embedding, dimension =300)    \n\nembedding_layer_fasttext_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_fasttext_pretrained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Deep Learning Models <a class=\"kk\" id=\"4\"></a>\n[Back to Contents](#0.1)\n\n We have initialized Keras embedding layer for our various word embedding models. Now it’s time to train using deep learning models. I will demonstrate how to train for glove pertained layer. You can test with other six embedding layer also (by just reassigning embedding_layer). One point you will notice that pretrained embedding layers performs much better than their trained counter parts. Again the purpose here is to depict basic Deep Learning model performance and not to obtain high score. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare embeeding layer of your choics \nembedding_layer = embedding_layer_glove_pretrained\n\n# can try with other embedding layes too\n# embedding_layer_fasttext_pretrained\n# embedding_layer_fasttext_trained\n# embedding_layer_cbow_trained\n# embedding_layer_sg_trained\n# embedding_layer_w2vec_pretrained\n# embedding_layer_glove_trained","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Basic DNN <a class=\"kk\" id=\"4.1\"></a>\n"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense\ndnn_model = Sequential()\ndnn_model.add(embedding_layer)\ndnn_model.add(Flatten())\ndnn_model.add(Dense(1, activation='sigmoid'))\n\ndnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\ndnn_model.summary()\n\nhistory = dnn_model.fit(x_train,  y = targets,\n                    epochs=10,\n                    batch_size=32,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  4.2 CNN <a class=\"kk\" id=\"4.2\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"import keras\ncnn_model = Sequential()\n# note : below we add embeeding layer\ncnn_model.add(embedding_layer)\ncnn_model.add(keras.layers.Dropout(0.2))\ncnn_model.add(keras.layers.Conv1D(3,3, padding='valid',activation='relu', strides=1))\ncnn_model.add(keras.layers.GlobalMaxPooling1D())\ncnn_model.add(keras.layers.Dense(20))\ncnn_model.add(keras.layers.Dropout(0.2))\ncnn_model.add(keras.layers.Activation('relu'))\ncnn_model.add(keras.layers.Dense(1))\ncnn_model.add(keras.layers.Activation('sigmoid'))\n\n# Get model summary\ncnn_model.summary()\ncnn_model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\n# compile the model\nhistory = cnn_model.fit(x_train,  y = targets,\n                    epochs=10,\n                    batch_size=32,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 RNN <a class=\"kk\" id=\"4.3\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN\nrnn_model = Sequential()\n# note : below we add embeeding layer\nrnn_model.add(embedding_layer)\nrnn_model.add(SimpleRNN(32))\nrnn_model.add(Dense(1, activation='sigmoid'))\nrnn_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = rnn_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4  Recurrent Neural Network -LSTM <a class=\"kk\" id=\"4.4\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"from keras.layers import LSTM\nlstm_model = Sequential()\n# note : below we add embeeding layer\nlstm_model.add(embedding_layer)\nlstm_model.add(LSTM(32))\nlstm_model.add(Dense(1, activation='sigmoid'))\n\nlstm_model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = lstm_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 Recurrent Neural Network – GRU <a class=\"kk\" id=\"4.5\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"from keras.layers import GRU\ngru_model = Sequential()\n# note : below we add embeeding layer\ngru_model.add(embedding_layer)\ngru_model.add(GRU(32))\ngru_model.add(Dense(1, activation='sigmoid'))\n\ngru_model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = gru_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Target Prediction  "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# lets predict target values for test set\nmodel =  dnn_model\nraw_preds = model.predict(x_test)\npreds = raw_preds.round().astype(int)\npreds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1088431/too%20much.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587453622&Signature=g%2Bd5%2FCpZZM9qmeHSRKrhXa2%2Bd0CpoKhJXYWBv2N1eyHQwVQeCY71D7pc8v1fcUFa1Zxw34b9lYkqfKZKbl5eMOwl6kVyY11RhLJymiCZNyUiuwupQoOubS18mXair9O1YflysMEwAi2vJut0eeuP6IlSd74V%2BfJYQMESOzjA4T1dgQGU%2Fc2i13zN0kVHs%2B66CAMkofE5u126GZ6VvmO13zrBsV4ruzJvEke4Tzqoh2u8T9iPKu3NHUVK8zxPtu75jsGJoQlVKBaTYprF7dZixji1QcZA8qftwGQ7YpSImOEVgOozyP%2Bwdpo1W%2FiqhwMN4mbqj8EBG1tOj43CUHklJA%3D%3D\" width=\"300\">\n\n\nThanks for reading! Purpose of this notebook is to provide fair understanding of word embedding techniques and help get beginners started in quick time. Do give your feedback. It will help me improve. In the next [part-3](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-3-bert-others) we will read about state of art  'BERT Embedding' along with few other NLP concepts.\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## References\n- Deep Learning with Python by FRANÇOIS CHOLLET  http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf\n- https://en.wikipedia.org/\n- https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/eb9cd609-e44a-40a2-9c3a-f16fc4f5289a.xhtml\n- https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html\n- https://www.kaggle.com/c/quora-question-pairs/discussion/31257#177483\n- https://www.kaggle.com/slatawa/simple-implementation-of-word2vec\n- https://becominghuman.ai/how-does-word2vecs-skip-gram-work-f92e0525def4\n- https://www.thinkinfi.com/2019/06/single-word-cbow.html(image)\n- https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer\n- https://medium.com/@japneet121/word-vectorization-using-glove-76919685ee0b\n- https://www.kaggle.com/christofhenkel/fasttext-starter-description-only\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}