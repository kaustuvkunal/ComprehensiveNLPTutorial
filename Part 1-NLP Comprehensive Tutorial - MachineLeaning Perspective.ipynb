{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is comprehensive tutorial about Natural Language Processing written for beginners and intermediate level users. The tutorial spans across 3 parts. Part-1 discusses NLP with respect to traditional Machine Learning perspective. Part 2 explains NLP with respect to Deep Learning perspective and Part 3 on NLP state-of-art 'BERT' concepts. I will be using [Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) dataset for our text modeling. Tutorial is WIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"kk\" id=\"0.1\"></a>\n",
    "## Contents\n",
    "\n",
    "1. [NLP Introduction](#1)\n",
    "1. [Text Cleaning](#2)\n",
    "    1. [Text Standardization](#2.1)\n",
    "        1. [Convert to Lower Case](#2.1.1)\n",
    "        1. [Spelling Correction](#2.1.2)    \n",
    "    1.  [Eliminate Undesirable Items From Text](#2.2)\n",
    "         1. [Removing Additional Spaces](#2.2.1)\n",
    "         1. [Removing Punctuations](#2.2.2)\n",
    "         1. [Removing URLs](#2.2.3)\n",
    "         1. [Removing Digits](#2.2.4)\n",
    "         1. [Removing Stopwords](#2.2.5) \n",
    "    1. [Convert Non-Words to Words](#2.3)\n",
    "        1. [Convert Emoji Into Words](#2.3.1)  \n",
    "    1. [Convert Negative Word to its Antonyms](#2.4)\n",
    "    1. [Dealing With Base and Derived Words](#2.5)\n",
    "         1. [Stemming](#2.5.1)\n",
    "         1. [Lemmatization](#2.5.2)\n",
    "    1. [Extract Text Using BeautifulSoup](#2.6)    \n",
    "1. [Text to Numeric Conversion](#3)\n",
    "    1. [Pre Conversion](#3.1)\n",
    "        1. [Corpus](#3.1.1)\n",
    "        1. [Tokenization](#3.1.2)\n",
    "        1. [Bag of Words](#3.1.3)\n",
    "        1. [N-Gram](#3.1.4)\n",
    "    1.  [Conversion](#3.2)\n",
    "         1. [Count Vectorization](#3.2.1)\n",
    "         1. [TF-IDF Vectorization](#3.2.2)    \n",
    "    1. [Post Conversion Dimesionality Reduction](#3.3)\n",
    "        1. [SVD(TruncatedSVD)](#3.3.1)         \n",
    "1. [ML-Modeling](#4)\n",
    "    1. [Naive Bayes Classifer](#4.1)\n",
    "        1. [Gaussian Classifier](#4.1.1)\n",
    "        1. [Bernoulli  Classifier](#4.1.2)\n",
    "    1. [Logistic Regression](#4.2)\n",
    "    1. [SVM](#4.3)\n",
    "    1. [XGBoost](#4.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLP Introduction <a class=\"kk\" id=\"1\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "Natural Language processing enables computer to understand and process human languages which include both text and audio. In this tutorial we will look into text processing.  \n",
    "\n",
    "pre-requsite:  Python and its libraries.\n",
    "tags: tutorial, begineer, learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Cleaning <a class=\"kk\" id=\"2\"></a>\n",
    "\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "Primary step of any Machine-Learning project is data cleaning. In text processing also, first we need to clean and standardise text. Lets look into some of the ways of cleaning the textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Text Standardization <a class=\"kk\" id=\"2.1\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Convert to Lower Case <a class=\"kk\" id=\"2.1.1\"></a>\n",
    "\n",
    "Words with different cases are intercepted differently such as 'The' and 'the'. Hence all words should be converted into same case, preferably lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome to nlp tutorial\n"
     ]
    }
   ],
   "source": [
    "text =\"Welcome to NLP Tutorial\"\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Spelling Correction <a class=\"kk\" id=\"2.1.2\"></a>\n",
    "\n",
    "At times textual data such as social media data is prone to spelling errors. Spelling errors should be rectified early during the clean-up phase. Fortunately we have libraries available for spelling correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling correction is properly perform\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "def correct_spellings(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_words.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "        \n",
    "\n",
    "\n",
    "text = \"Spelling correctin is proprly perfrm\"\n",
    "text = correct_spellings(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Eliminate Undesirable Items From Text <a class=\"kk\" id=\"2.2\"></a>\n",
    "\n",
    "Texts consist of items that are not useful with respect to text processing and it is better to eliminate them before modelling. Few such items that should be considered for removal and code to remove them are given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1  Removing Additional Spaces <a class=\"kk\" id=\"2.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting double space text \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Correcting   double  space  text \"\n",
    "text = re.sub(' +', ' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Removing Punctuations <a class=\"kk\" id=\"2.2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This scententance has so many punctuations\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"This! scententance, has so: many- punctuations.\"\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Removing URLs  <a class=\"kk\" id=\"2.2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I refer this answer in  ?\n"
     ]
    }
   ],
   "source": [
    "text = 'Shall I refer this answer in www.google.com ?'\n",
    "text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4.  Removing Digits <a class=\"kk\" id=\"2.2.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Being no   team is more important or being no   but with fair play \n"
     ]
    }
   ],
   "source": [
    "text =\"Being no 1 team is more important or being no 3 but with fair play \"\n",
    "text= re.sub(r'[0-9]',' ',text)\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5. Removing Stopwords  <a class=\"kk\" id=\"2.2.5\"></a>\n",
    "\n",
    "Stopwords are the most common words in a language. For example 'is', 'the', 'that' etc. are stopwords in English language. Stopwords shall be removed during text clean-up phase. However removing stop word can change the meaning of sentence. For instance 'I didn't love politics' will get converted to 'I love politics' after removing stopword.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This important topic\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "text = \"This is not the most important topic\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # stop_words will contain  set all english stopwords\n",
    "    filtered_sentence = []   \n",
    "    for word in text.split(): \n",
    "        if word not in stop_words: \n",
    "            filtered_sentence.append(word) \n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "text = remove_stopwords(text)\n",
    "print(text) \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Convert Non-Words to Words <a class=\"kk\" id=\"2.3\"></a>\n",
    "\n",
    "Special symbols such as emoticon, emojis etc. are example of non-words.  These non-words should be either converted into words or removed from text. Emoji library converts emojis to equivalent words as shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Convert Emoji Into Words  <a class=\"kk\" id=\"2.3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :thumbs_up:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "text = 'Python is üëç'\n",
    "print(emoji.demojize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Convert Negative Word to its Antonyms <a class=\"kk\" id=\"2.4\"></a>\n",
    "\n",
    "Negative words should be replaced with their antonym for efficient processing. For instance 'not good‚Äô should be replaced with bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was unhappy with the score of team\n"
     ]
    }
   ],
   "source": [
    "text = \"He was not happy with the score of team\"\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "def convert_to_antonym(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = []\n",
    "    temp_word = ''\n",
    "    for word in words:\n",
    "        antonyms = []\n",
    "        if word == 'not':\n",
    "            temp_word = 'not_'\n",
    "        elif temp_word == 'not_':\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for s in syn.lemmas():\n",
    "                    for a in s.antonyms():\n",
    "                        antonyms.append(a.name())\n",
    "            if len(antonyms) >= 1:\n",
    "                word = antonyms[0]\n",
    "            else:\n",
    "                word = temp_word + word # when antonym is not found, it will\n",
    "                                    # remain not_happy\n",
    "            \n",
    "            temp_word = ''\n",
    "        if word != 'not':\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "    \n",
    "text = convert_to_antonym(text)\n",
    "print(text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Dealing With Base and Derived Words <a class=\"kk\" id=\"2.5\"></a>\n",
    "\n",
    "Words can be classified into Base word and Derived word. For example 'go' is a base word and  'going', 'gone' and 'went' are its derived words. Preferably during data cleaning phase derived word shall be converted to their base counterparts. There are two ways of finding base word of any word - Stemming and Lemmatization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Stemming  <a class=\"kk\" id=\"2.5.1\"></a>\n",
    "\n",
    "Stemming is a rule base technique. In Stemming Base word is identified by chopping the word at end. For instance 'going' and 'gone' will get converted to 'go' but 'went' will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david want to go with alfa but alfa went with charli so david is go with bravo\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "text = \" David wanted to go with Alfa but Alfa went with Charli so David is going with Bravo\"\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "text = stem_words(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Lemmatization   <a class=\"kk\" id=\"2.5.2\"></a>\n",
    "Lemmatization is a dictionary based technique and more accurate than stemming. It looks up to the dictionary to fetch Base word (called as lemma). The obvious downside is, it is slow in processing because has to store and look up the dictionary.\n",
    "\n",
    "Additionally, lemmatization requires Parts of speech tagging. Lets understand POS tagging first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Parts of Speech (POS) tagging \n",
    "\n",
    "Parts of Speech Tagger processes a sequence of words and attaches a part of speech tag to each word. 'nltks poc_tag' library is used for POS tagging. Some POS tags examples are VBZ -> Verb, NN-> Noun, PRP -> preposition, IN -> Interjection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('very', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('observation', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('you.', 'NN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"This is very good observation by you.\"\n",
    "nltk.pos_tag(text.split()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you want to know what exactly each tag specify use nltk help function as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /Users/kaustuv/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('DT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now coming back to Lemmatization, we need to define a wordnet_map (as in below code) and specify for which all parts of speech we need to find Base words otherwise by default it will fetch base words for nouns only.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'David want to go with Alfa but Alfa go with Charli so David be go with Bravo'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV }\n",
    "\n",
    "# without wordnet map it takes evey word as noun\n",
    "text = \"David wanted to go with Alfa but Alfa went with Charli so David is going with Bravo \"\n",
    " \n",
    "def lemma_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word ,wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "\n",
    "lemma_words(text) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Extract Text Using BeautifulSoup <a class=\"kk\" id=\"2.6\"></a>\n",
    "Beautiful Soup is a very useful Python library used for extracting text data from HTML and XML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html><head><title>The NLP story</title></head>\n",
      "<body>\n",
      "<p class=\"title\"><b>The NLP story</b></p>\n",
      "<p class=\"story\">Once upon a time there were three little  techniques; and their names were\n",
      "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
      "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
      "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
      "and they lived till the next conference.</p>\n",
      "<p class=\"story\">...</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "<html><head><title>The NLP story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The NLP story</b></p>\n",
    "<p class=\"story\">Once upon a time there were three little  techniques; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived till the next conference.</p>\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NLP story\n",
      "\n",
      "The NLP story\n",
      "Once upon a time there were three little  techniques; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived till the next conference.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "text = BeautifulSoup(text, \"html\").text# HTML decoding\n",
    "# for lxml  decodinf\n",
    "#text = BeautifulSoup(text, \"lxml\").text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we know possible ways to clean our text data we will write a clean-up function for our Disaster Tweets dataset.\n",
    "\n",
    "Note that:  which all of the text clean up approaches you will use is mainly depend upon problem domain, dataset and individual perception. For instance for newspaper and journal articles we might not need any spelling correction. Clean-up function is an important factor in deciding overall classification outcome.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "  \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text= re.sub(r'[^\\w\\s#]',' ',text) #Removing every thing other than space, word and hash\n",
    "    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n",
    "    text= re.sub(r'[0-9]',' ',text)\n",
    "    #text = correct_spellings(text)\n",
    "    text = convert_to_antonym(text)\n",
    "    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7fbd32b48a12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlp-getting-started/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlp-getting-started/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4043\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8d7e5d95e949>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lowercase text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\w\\s#]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Removing every thing other than space, word and hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtext\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"https?://\\S+|www\\.\\S+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[0-9]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "train_df = pd.read_csv('nlp-getting-started/train.csv')\n",
    "test_df = pd.read_csv('nlp-getting-started/test.csv')\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text to Numeric Conversion <a class=\"kk\" id=\"3\"></a>\n",
    "\n",
    "[Back to Contents](#0.1) \n",
    "\n",
    "Once text is cleaned we need to feed it into learning Algorithm. Learning algorithms understand numbers and not words hence we will convert text into numbers before feeding into any modeling algorithm. \n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1076574/texttonumber.jpg?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587022133&Signature=ZKEMQXWY2SMdSGxrwIZVOte5sqVB5eJdk4QqUpZdv0M8ABHnaNHlUdisCpJck1owJS9UYRXrSXTz%2BuGfVlebU6KPyjTpxQ5zTX2A8WBKQxy7X6QmlKBiI60KRbffWTg8cTgI4GYQQk1G6rxf9CYLGJaRjeXwFRXpyWbeoGCSPMl3wn43oJd2ojrJQwOh4t%2BIujVYy1tZG8taQ5ryP53Xfc9%2FLyCLhlW6p4I7GUWPVRGz1BtYqPEgcbT%2FPOV%2BxcHEL4Yux1%2F%2BsG0Ce%2BGhrwOn%2FUopHJ4nZDNp7aAZsY4RkTUt15edPNTQkmhmLhS4dZHnYGwYnvrk7DxfQGTi12FTVQ%3D%3D\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pre Conversion <a class=\"kk\" id=\"3.1\"></a>\n",
    "\n",
    "Before performing text to numeric conversion we will look into few common NLP vocabulary terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Corpus <a class=\"kk\" id=\"3.1.1\"></a>\n",
    "Collection of all available textual data is known as corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= pd.DataFrame(columns=['text'])\n",
    "corpus['text']= pd.concat([train_df[\"text\"], test_df[\"text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Tokenization<a class=\"kk\" id=\"3.1.2\"></a>\n",
    "\n",
    "Text is segmented into its smaller parts called tokens. This segmentation process is known as tokenization. A common example of tokens we can think of is words. However tokens can be something else also such as combination of 2 words. \n",
    " <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1076574/tokenization.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587023230&Signature=cc3O1hxGA1QYe6jLoLr3B7lRKVRg3TIlX%2FKG9YAUXt1VLo1mhY03HKhbg8eXjAK4JKxQ0yAiDCH3dMAApUKs%2F%2FbpBoKDGCGJ%2Fn%2FdaX71uH%2B6hwXYGIHC4AFFbD9Fz%2B0gK91WT%2F9ByzMqFUJrOc2mPqemqYDOnLSiRzH7EwzaTfak7I63vl25Ok4DAyay1BPiM8%2FtAYX8dkSgW2MTjgDmJyYmwHo9yJTfu2pTwt6wLL6em8BdMVvDzsXIfsL0ZTbJ5KeQFdioFWThuXrp5hoov023A6aUJaCgNIxvPngLl5PDvGaxN26FypkQSuhGV245FbJtSWya1PsIykgJ779DKw%3D%3D\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.kaggle.com/kksienc/images/download/q0AJ6bb0CyqpQD97dK4Y%2Fversions%2F7G9dvbfyieawNLGYkjk2%2Ffiles%2Fskipgram.png?datasetVersionNumber=2\" width=\"2350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Bag of Words (BOW) <a class=\"kk\" id=\"3.1.3\"></a>\n",
    "\n",
    "BOW approach is a way of representing text data. BOW approach recognizes text as just a bag of words and keeps a count of the total occurrences of words in text. It does not consider meaning or context of the word in the document. For example,\n",
    "\n",
    "- Text: \"Now tell me BOW approach is good or not so good?\"\n",
    "- BOW : \"Now\",\"tell\",\"me\",\"BOW\",\"approach\",is\",\"good\",\"or\",\"not\",\"so\",\"good\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 N-Gram <a class=\"kk\" id=\"3.1.4\"></a>\n",
    "\n",
    "N-grams is a contiguous sequence of n words from a given sample of source text. For Example: \n",
    "\n",
    " <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1077794/ngram.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587024775&Signature=AVNDCKpaEtD4OY3mmx6Tlgux3FdWDm5H9FFtksQ13JYKPD%2FgE%2Baq%2BnSgom4L1DZ2wZHy02%2BnwT5hU6szqlm54NmHVuIrc6%2FLaTp3y3wz37xiU2O2tJBwaUOk4kZYuK4rns66vjozYv4lmhJ0f841JlyoE%2FetxjziCKQb%2FQ4Ot6E%2B810vJREkTjBnlwlDalxV%2FrRXbN%2FipQw6N5G4OJlxXD3UcVgAQ8T%2FuyLergkcaoWq9npylvdnU7c7tV8YU%2FVEjIlQFaFgjBJWt3clA4PadrItM7CEa%2BNH2jDi0DF9KeMeuRHBRfRx1klfdpqlT9rk1jt3w%2FBJ7pHZbXGeIv25ZA%3D%3D\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Conversion <a class=\"kk\" id=\"3.2\"></a>\n",
    "In this section we will look into text to numerical conversion techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 CountVectorization  <a class=\"kk\" id=\"3.2.1\"></a>\n",
    "\n",
    "CountVectorization converts a collection of text documents to a matrix of token counts. It counts the tokens in text.  Total no of tokens i.e. vocabulary size will equal to matrix column length where each column represents a token. A row array represents unique tokens and the number at a cell will indicate count of that token in the record.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1078299/countvectorization.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587042006&Signature=icucJjhaFWcEF4cFABp3X4zP%2FamO8sBnTIZoeb8pgZVulkOBzmo%2BqW7%2F1prpWfl0Lm7dNchJmmOnLsMT3ZZEHd4R%2FPnuKWfanDF6VEYmGACWDmAMu%2BYdAR5ZMjYdSJ0hk5Au%2Fh%2B8Bu7qdq6qtFTC0OLZxWvBCd3TX4ND6JKRE2wJgvn3eOPJ%2B7OX%2Fm8cGTKUc9cJLsNeLBzXMLGrBpfj%2BUNy%2FassVFOQtewc0CdZqv3xtDy%2BmAjilHO08GKmCHcGiPQ8WngwClzjqr5QeUKP0n6vNWrOsZeWZGUHqLBAxXjUIGrn0WMEXGQW2u52XQsU9f8c7QqA0LFd8gdU8kAOUg%3D%3D\" width=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "# analyzer should be word/ character\n",
    "#ngram_range lower and upper boundary of the range of n-values\n",
    "\n",
    "## let's get counts for the first 5 tweets in the data\n",
    "train_countvectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "# generating test CountVectorizer matrix\n",
    "test_countvectors = count_vectorizer.transform(test_df[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 22331)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_countvectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 22331)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_countvectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 22331)\n",
      "  (0, 13799)\t1\n",
      "  (0, 4318)\t1\n",
      "  (0, 867)\t1\n",
      "  (0, 18796)\t1\n",
      "  (0, 15646)\t1\n",
      "  (0, 13419)\t1\n",
      "  (0, 18906)\t1\n",
      "  (0, 5273)\t1\n",
      "  (0, 11625)\t1\n",
      "  (0, 508)\t1\n",
      "  (0, 6682)\t1\n",
      "  (0, 20018)\t1\n",
      "  (0, 507)\t1\n",
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# converting sparce to dense vector \n",
    "print(train_countvectors.shape)\n",
    "print(train_countvectors[0])\n",
    "print(train_countvectors[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 TF-IDF Vectorization <a class=\"kk\" id=\"3.2.2\"></a>\n",
    "\n",
    "TF-IDF stands for 'Term frequency-inverse document frequency'. It measures importance of a token (called as term) with respect to its record (called as document) in a corpus. Every term of a document is assigned a weight after multiplying its term frequency (tf) and inverse document frequency (idf).\n",
    "\n",
    "\n",
    " <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1078299/tfidf.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587042045&Signature=ScsX6XYgFgKakywVho2acjOkv1cp09XbGGemdOMEFKI3bTE5YwseL0KPKtBdHoWvabJcY7ukxuG5S1wafGDHS9BTkPLsdsNIN9xUb9KxZKOwl2QgSzeY7%2BjhQbpMwvXymOPmH9TYaL2oWIKvB%2FTrOL7HsWKjQUNDSBzG%2BZne6jtqNUrfZOaESp5SZG51fvIw2bWi2V4w3cDGT%2BdN3rfevnuXAU1yulehDefQqBUyUHBmalPrUnvtgMhhIY4iK4ZOm23%2BdF0LVZVA8tSiILKq%2B6qt%2FuluT7iytg1%2Fq17M1Qm6kFNWaXpozJlD%2F98AfVeRVz7eMjWXeKhLkIlZIXqoow%3D%3D\" width=\"350\">\n",
    "\n",
    "Internally different libraries use slightly different formula to calculate 'idf' value though underlying idea remains same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words='english', ngram_range=(1, 1))\n",
    "# training tfidf on corpus\n",
    "tfidf_vectorizer.fit(corpus['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 28563)\n",
      "(3263, 28563)\n",
      "(7613, 28563)\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_tfidfvectors = tfidf_vectorizer.transform(train_df['text'])\n",
    "test_tfidfvectors = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "print(train_tfidfvectors.shape)\n",
    "print(test_tfidfvectors.shape)\n",
    "print(train_tfidfvectors.todense().shape)\n",
    "print(train_tfidfvectors[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Post Conversion <a class=\"kk\" id=\"3.3\"></a>\n",
    "\n",
    "Both Countvectorization and tf-idf technique produce large size matrix. Post text-to-numerical conversion dimensionality reduction techniques can be tried out to reduce matrix size though keeping their relative importance intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 SVD ( TruncatedSVD)   <a class=\"kk\" id=\"3.3.1\"></a>\n",
    "\n",
    "TruncatedSVD transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD).It efficiently works on sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 22331)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_countvectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD   \n",
    "  # indicating varience percentage  \n",
    "tsv = TruncatedSVD(n_components=100)\n",
    "train_countvectors_svd = tsv.fit_transform(train_countvectors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_countvectors_svd = tsv.fit_transform(train_countvectors) \n",
    "train_tfidfvectors_svd = tsv.fit_transform(train_tfidfvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_countvectors_svd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML Modeling <a class=\"kk\" id=\"4\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "Now our text data is in numerical form and is ready to feed into ML Algorithm. We'll train classifier models with our dataset. Purpose here is to depict basic model performance and not to obtain high score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base function to train madel against  datasets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning))\n",
    "from sklearn import model_selection\n",
    "\n",
    "def text_modeling( model):\n",
    "    print( \"Model :\"+ str(model))\n",
    "    print('***** F1 Scores *******')\n",
    "    scores=model_selection.cross_val_score(model, train_countvectors.toarray(), train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "    print(\"CountVectorized dataset :\"+str(scores.mean()))\n",
    "    scores= model_selection.cross_val_score(model, train_tfidfvectors.toarray(), train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "    print(\"TF-IDF Vectorized dataset :\"+str(scores.mean()))\n",
    "    scores = model_selection.cross_val_score(model, train_tfidfvectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "    print(\"TF-IDF Vectorized + SVD dataset \"+str(scores.mean()))\n",
    "    scores = model_selection.cross_val_score(model, train_countvectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "    print(\"CountVectorized + SVD dataset \"+str(scores.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Naive Bayes Classifier <a class=\"kk\" id=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Gaussian Classifier <a class=\"kk\" id=\"4.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gn = GaussianNB()\n",
    "text_modeling( gn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Bernoulli  Classifier <a class=\"kk\" id=\"4.1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "br = BernoulliNB()\n",
    "text_modeling( br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Logistic Regression <a class=\"kk\" id=\"4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "text_modeling( lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Support Vector Machine <a class=\"kk\" id=\"4.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(probability=True )\n",
    "text_modeling(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 XGBoost <a class=\"kk\" id=\"4.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "text_modeling(xgb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That‚Äôs all in this part.\n",
    "In the next part we will see how to take care of synonyms and language specific things. We will learn about one of the important concepts of NLP 'Word Embeddings' and see how Deep Learning has simplified NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://en.wikipedia.org/\n",
    "\n",
    "- https://www.tutorialspoint.com/python_text_processing/python_spelling_check.htm\n",
    "\n",
    "- https://www.kaggle.com/praga95/nlp-tutorial/\n",
    "\n",
    "- https://becominghuman.ai/word-vectorizing-and-statistical-meaning-of-tf-idf-d45f3142be63\n",
    "\n",
    "- https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "\n",
    "- https://docs.python.org/3/library/re.html\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "- https://www.kaggle.com/ktattan/lda-and-document-similarity\n",
    "\n",
    "- https://emilhvitfeldt.github.io\n",
    "\n",
    "- https://gdcoder.com/nlp-transforming-tokens-into-features-tf-idf/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
