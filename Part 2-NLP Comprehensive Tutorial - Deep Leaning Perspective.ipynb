{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is part-2 of [Comprehensive tutorial on NLP](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-1-ml-perspective). In this part we will learn about word embedding and see how Deep learning has simplified NLP processing. \n",
    "\n",
    "Pre-requsite: Basic Deep learning undestanding would be helpful though not mandatory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"kk\" id=\"0.1\"></a>\n",
    "## Contents\n",
    "\n",
    "1. [Introduction to Word Embedding](#1)\n",
    "    1. [Dimensionality](#1.1) \n",
    "    1. [Padding](#1.2)\n",
    "    1. [Euclidean Distance](#1.3)\n",
    "    1. [Cosine Similarity](#1.4)\n",
    "1. [Word Embedding Techniques](#2)\n",
    "    1. [Word2Vec](#2.1)\n",
    "        1. [Skip-Gram](#2.1.1)\n",
    "        1. [CBOW (Continuous Bag of Words)](#2.1.2)\n",
    "    1. [GloVe](#2.2) \n",
    "    1. [FastText](#2.3)\n",
    "1. [Text to Numeric Convertion Using Word Vectors](#3)\n",
    "    1. [Vector Averaging](#3.1)\n",
    "        1. [Vector Averaging With Word2Vec](#3.1.1)\n",
    "        1. [Vector Averaging With GloVe](#3.1.2)\n",
    "        1. [Vector Averaging With FastText](#3.1.3) \n",
    "    1. [Embedding Matrix & Keras Embedding layer](#3.2)\n",
    "        1. [Word2Vec Embedding layers](#3.2.1)\n",
    "        1. [GloVe Embedding layers](#3.2.2)\n",
    "        1. [FastText Embedding layers](#3.2.3)      \n",
    "1. [Deep Learning models](#4)\n",
    "    1. [CNN](#4.1)\n",
    "    1. [Simple RNN](#4.2)\n",
    "    1. [Recurrent Neural Network -LSTM](#4.3)\n",
    "    1. [Recurrent Neural Network – GRU](#4.4)\n",
    "    1. [Bidirectional RNN](#4.5)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Word Embedding  <a class=\"kk\" id=\"1\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "Word Embedding is also known as Word Vectorization. It means converting word into vector. Vectors are numeric representation of a point in space. Mathematically vectors are 1D array or sequence of numbers.  \n",
    "\n",
    "\n",
    "<B>Why we need Word Embedding? </B>\n",
    "\n",
    "A problem with our previous text to numeric conversion techniques was that they ignore synonyms for example word 'measure' and ‘calculate’ were represented differently however in most sentences they can be used interchangeably. In Word Embedding similar words are spatially close to each other in vector space. Word Embedding is also capable of preserving semantic and syntactic similarity and relation with other words. The vector representation are such that geometric transformation adopts syntax and semantic. For instance, by adding a “female” vector to the vector “king”, we obtain the vector “queen” and by adding a “plural” vector to the vector “king”, we obtain “kings”. \n",
    "\n",
    "Another problem we observe in part 1 was production of high dimensionality sparse matrix. Word Embedding produces low dimensionality dense matrix.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1078299/textproc.jpg?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587069821&Signature=Gu0D8VdPGbHqYR0hgSyEaPbZlM95%2FIUtleAJeZVSiF4cu8%2FwVFtOdIjebA0SkD%2BBTxwEdu6gFhtbvIszo8diz22CsNNQ4oNdR0qNY3IjKgtYiy1qkA3bt5ExfeA%2BpH1TAOnDmMFqUZ9JvO5f7x1c4SfjM5aN269zGUESZJQZwMJJl1CWlYYrEtJALpXUYrk8gZ%2BKwucdEipTInTQNJG1qR32bInzOk7nN88DUrc6kxfd0aZn7mN%2FP%2FZ87d4JMdJ3ul7hAJm42vPEmFe4pbLDR1k9xwGYzlN1AM0cVJs6M2Z7StFJ4uSMEDhc5Iil6xn%2BbHroEZyoBVGr7rjWMPf6Rg%3D%3D\" width=\"250\">\n",
    "\n",
    "Before applying Word Embedding techniques lets look into into few common NLP vocabulary terms.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dimensionality  <a class=\"kk\" id=\"1.1\"></a>\n",
    "\n",
    "Dimensionality refers to the length of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Padding <a class=\"kk\" id=\"1.2\"></a>\n",
    "Padding is task of appending string up to given specific length with whitespaces. Padding is used to represent all records as fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Euclidean Distance <a class=\"kk\" id=\"1.3\"></a>\n",
    "\n",
    "Euclidean distance is the shortest distance between two points in (Euclidean) space.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1078299/eucldeandistance.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587070547&Signature=ND5YK6rJrRHMwJ4erD9fFQCfBYlL2YIcWVNQJsi1o%2FK9RgGgj6egkkqdlcHsBShwx%2B9uovT1Fwwnv9xnGhKigjdtMHHWA%2F8%2Fi2E3qK3iScr2o9iDYjv5WXQ0WeXMV8AUq%2F1nWMqCLPsSfwB0hc%2FrgVrHi5xtOxBaGlt%2BP7X73d8cSK4WoPBI%2FhJswcqruQFDxrO2%2BvIupOzibAC6XBLQ%2BM%2BS0H0Fpuyxke45F%2B6qqsLpyFzNUFfzQzUXM6n6JvLMUZVLEjP%2BaikoujgvFZ9AmFtljGZ%2FOzKuYWNdFtuvZ55aQkLAPI9yU%2FlUDmI45y%2Bq%2BO43e7ql%2FhsDPyhcaG%2Fh8g%3D%3D\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Cosine Similarity  <a class=\"kk\" id=\"1.4\"></a>\n",
    "Cosine similarity is a measure of similarity between two nonzero vectors of an inner product space. It measures the cosine of the angle between them.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1078299/cosineSimilarity.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587070566&Signature=iebe54mjBjdPju%2Bl5KwvX5iJTg0jpt%2FH%2FvQUmGuCxpG1t0zoPlmugIHc%2ByBnoLf3iu8yjxg%2FdK7Qj6G6iSS3JsQjt%2FEwavxrB3IjOHH0%2Fu3%2F3vE8Q1cY4tZSEjTI4u98VuDbpYUUPHa%2FRSe6IE2%2BVhZxeaQ1jcSI8LshJncCCrmS9IOxr4U2vN%2F7q6hfXJgqIin6jO%2B0PX%2BeUw1PNa%2BObhmve4zeA2rJ4qzhEnoMY6mF6vRm%2Bf3%2FsMew1crd67GVzoMFlmwisfS6g%2Fb2GR620YjoPL5cZ6XrxAmFnZQEqh2ur%2FGvpRfcPRCm1rrL0N%2FQc%2FAm7fRXbiMibWNqi%2FBD6w%3D%3D\" width=\"250\">\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embedding Techniques <a class=\"kk\" id=\"2\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "Now we will look into word Embedding techniques but before that let's fetch our [Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) dataset and clean it as we did in part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Fetch & Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data  make it ready \n",
    "#!pip install pyspellchecker\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk \n",
    "import re\n",
    "\n",
    "train_df = pd.read_csv(\"nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"nlp-getting-started/test.csv\")\n",
    "\n",
    "\n",
    "def convert_to_antonym(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = []\n",
    "    temp_word = ''\n",
    "    for word in words:\n",
    "        antonyms = []\n",
    "        if word == 'not':\n",
    "            temp_word = 'not_'\n",
    "        elif temp_word == 'not_':\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for s in syn.lemmas():\n",
    "                    for a in s.antonyms():\n",
    "                        antonyms.append(a.name())\n",
    "            if len(antonyms) >= 1:\n",
    "                word = antonyms[0]\n",
    "            else:\n",
    "                word = temp_word + word # when antonym is not found, it will\n",
    "                                    # remain not_happy\n",
    "            \n",
    "            temp_word = ''\n",
    "        if word != 'not':\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def correct_spellings(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_words.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "        \n",
    "\n",
    " \n",
    " \n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "  \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text= re.sub(r'[^\\w\\s#]',' ',text) #Removing every thing other than space, word and hash\n",
    "    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n",
    "    text= re.sub(r'[0-9]',' ',text)\n",
    "    #text = correct_spellings(text)\n",
    "    text = convert_to_antonym(text)\n",
    "    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    \n",
    "    return text\n",
    "\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "sentences= pd.DataFrame(columns=['text'])\n",
    "sentences['text']= pd.concat([train_df[\"text\"], test_df[\"text\"]])\n",
    "\n",
    "from collections import defaultdict\n",
    "tokens_list = [row.split() for row in sentences['text']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Word2Vec    <a class=\"kk\" id=\"2.1\"></a>\n",
    "\n",
    " Word2Vec is group of related models that are used to produce Word Embeddings. It was created & patented by Tomas Mikolov and a group of a research team from Google in 2013. Each unique word in the corpus is assigned a corresponding vector in the space. Word2Vec relies only on local information of language hence the semantics learnt for a given word is only affected by the surrounding words. Underlying assumption of Word2Vec is that two words sharing similar contexts also share a similar meaning, this at times results into similar vector representation (cosine similarity) of multiple words.One more drawback of word2vec is its unablity to takecare of OOV word. \n",
    "\n",
    " <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1078299/word2vec.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587070589&Signature=Po7EpAiaeyI5eQz3PkX1JTE81LQNoCnvlfmMIEWqwkBEJBbN5ci6wGTrj0oIacrk8y95cz1x6LRqfeHhqXaw96D299LtueOXK%2FNRqsJRXUPJNuPl%2F%2BWT25kIC%2BChAnfWYrJ8h%2FHXkoUxylq8DRpAxFAxpvdUXivwY2vR6h3aqhstu26c%2FMGTFljD%2FXIl4I5aDv3m8agm8nx7UdVLJ0nGBjEKmuhzszbACwIkUi2SGyiVhZhWngP8yXg3KsLg3vtbuNF9LOkk%2BfBeLBCYAB7qwuQZhNcXRqDLHmepVPc0%2BlWHllfj1RYvxLjbjHTlry1n0Ovy9ELl9maLT8FLFIhVgA%3D%3D\" width=\"250\">\n",
    " \n",
    " we will see error when we try to buid  Word2Vec comes in two flavours,\n",
    " - Skip-Gram and \n",
    " - Continuous Bag of Words (CBOW)\n",
    "\n",
    "Underneath Word2Vec uses neural network algorithms that can be trained on any type of sequential data. Fortunately we have libraries available that have already implemented these algorithms and we have to just call the method with proper arguments. A popular one among such libraries is <B>gensim</B>. It provides the [Word2Vec Class](https://radimrehurek.com/gensim/models/word2vec.htm) for working with a Word2Vec model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gensim implementation of Word2vec\n",
    "\n",
    "\n",
    "\n",
    "<B>Arguments:</B>\n",
    "\n",
    "- min_count : Minimum number of occurrences of a word in the corpus to be included in the model. The higher the number, the less words we have in our corpus\n",
    "- window: The maximum distance between the current and predicted word within a sentence\n",
    "- size: The dimensionality of the feature vectors\n",
    "- workers: no of cores\n",
    "- sg = 1  for skipgram and 0 for cbow\n",
    "\n",
    "- sample = (type float) - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
    "\n",
    "- alpha = float - The initial learning rate - (0.01, 0.05)\n",
    "\n",
    "- min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "\n",
    "- negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "\n",
    "<B>Methods :</B> \n",
    "- model.build_vocab: Prepare the model vocabulary\n",
    "- model.train: Train word vectors\n",
    "- model.init_sims(): When we do not plan to train the model any further, we use this line of code to make the model more memory-efficient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1.1 Skip-Gram  <a class=\"kk\" id=\"2.1.1\"></a>\n",
    " \n",
    "Skip-Gram is designed to predict the context from base word. From a given word, Skip-gram model tries to predict its neighbouring words.  <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1078299/skipgram.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587070609&Signature=FN0mWWM%2Fuvc34xG2SZQx7VCLn70Ma58jAnw%2BP%2BO8Gtf4opQVvuu2iRkUArE6rNokzb0xfC3JcA1ybNHFAvJBQU%2FNjeY1cQLFKedxJ8TZMYPj%2FYx1tz5QJ8OvKTok%2FeDDPyr442dKbkuQDNsAawxo%2FRJXoWxqHFMAFZyOiK2UfjyVhA%2Fth8k9OeBhEaUi0slkIgeEp5oYi0BLrJmjVpYmh6tXstvK%2Bs43QxyZw98PDDZpKOZ6QX8o%2B9LbZR6ZsjfILc79CHXQHWvLjfKC%2FDKTe1RB%2ByEAc8XooPm5BaUTHXMYRYJS03hNo5NmvAb1anKSYX25dj6Ww2SrB0at6VS0dQ%3D%3D\" width=\"250\">\n",
    "\n",
    " Skip-gram is a [(target, context), relevancy] generator. Skip-gram generator gives us pair of words and their relevance (a float value). Lets generate Word2Vec skip-gram embedding for our cleaned-up text dataset using gensim.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building Skipgram  WordVectors using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build Skip gram model vocab: 0.12 mins\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from time import time\n",
    "t = time()\n",
    "# initialize skipgram model\n",
    "sg_model = Word2Vec(min_count=2,window=2,size=300, sg = 1,sample=5e-5, alpha=0.05, min_alpha=0.0005,negative=20 )\n",
    "# build model vocabulary\n",
    "sg_model.build_vocab(tokens_list)\n",
    "\n",
    "# train the model\n",
    "sg_model.train(tokens_list, total_examples=sg_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to build Skip gram model vocab: {} mins'.format(round((time() - t) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just build our first word-embedding model.and that also with only 3 lines of code. Lets play with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert a word  to vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sg_model['hope']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate dimension of our word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sg_model['hope'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Measure similarity   b/w two word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sg_model.similarity('people','saint' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_model.similarity('people', 'terrorist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fetch most similar words  wrt any given word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_model.most_similar('fire')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fetch list of word vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(sg_model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 CBOW (Continuous Bag of Words)  <a class=\"kk\" id=\"2.1.2\"></a>\n",
    "\n",
    "CBOW is designed to predict the base(target) word from context. CBOW is faster to train than the skip-gram and gives slightly better accuracy for the frequent words.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1078299/cbow.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587070646&Signature=O1By12DGolOFtiR2%2FsFF%2FJs9ECXlq3HilWLoy0fR6ypVPGKsZJZ5EhViXBU%2FgkK%2FwDcZ9scFexc3XktXgxJl2BjnQb3GGQuwUeU0y2oYuZwuVRitfGe86McgPrstbBcqKsehbjunpoyPwgRoiq3Hk7Fu5vaoKo7y0fBJTv0OziTT%2Bgwc9Uxc6WjS5SfAIrk1uW9OzFtYdRekgIbAgEyCHpqh6UrfHz9ePi2Ggpfbns42VP7oMRKQXHKU8LDs5vYuXi2ew9pUYmChDz8C9FV%2FfprQSj7oDqMxrKijZEyYXDbo4VgBMJI%2FC7lDzEn%2B060m5%2BwAVXhEgYS8KmXba%2BgS2w%3D%3D\" width=\"250\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build CBOW model vocab: 0.09 mins\n"
     ]
    }
   ],
   "source": [
    "#### Building CBOW wordvectors\n",
    "from gensim.models import Word2Vec\n",
    "from time import time\n",
    "t = time()\n",
    "# initialize\n",
    "cbow_model = Word2Vec(min_count=2,window=2,size=300, sg = 0,sample=5e-5, alpha=0.05, min_alpha=0.0005, \n",
    "                     negative=20 )\n",
    "# build model vocabulary\n",
    "cbow_model.build_vocab(tokens_list)\n",
    "\n",
    "# train the model\n",
    "cbow_model.train(tokens_list, total_examples=cbow_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to build CBOW model vocab: {} mins'.format(round((time() - t) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice CBOW trained faster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pretrain Word2Vec\n",
    "\n",
    "Google has made available pretrained word embedding which includes word vectors for a vocabulary of 3 million words and phrases that they have trained on roughly 100 billion words from Google News dataset using Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fetch  pretrain  Word2Vec model vocab: 1.42 mins\n"
     ]
    }
   ],
   "source": [
    "#fetching  pretrain wordvector\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "t = time()\n",
    "pretrained_w2vec_embedding = KeyedVectors.load_word2vec_format('/Users/kaustuv/DataScience/DS_tutorials/datasets/Google-Word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print('Time to fetch  pretrain  Word2Vec model vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_w2vec_embedding['people']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_w2vec_embedding.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GloVe  <a class=\"kk\" id=\"2.2\"></a>\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/pubs/glove.pdf) stands for \"Global Vectors\". It is a Word Embedding [project](https://nlp.stanford.edu/projects/glove/)  written in C language and developed by Stanford university researchers in 2014. Glove embedding technique is based on (first) construction of a co-occurrence matrix from a training corpus and then (second) factorization of co-occurrence matrix in order to yield word vector.\n",
    "\n",
    "Unlike word2vec which captures only local statistics of token Glove captures both global statistics and local statistics of a text tokens. Its embeddings relate to the probabilities that two words appear together. [glove_python](https://github.com/maciejkula/glove-python) library provides glove implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Implementation of Glove via  glove_python\n",
    "\n",
    "\n",
    "Arguments description : \n",
    "\n",
    "1. For corpus.fit()  :\n",
    "    - lines : this is the 2D array we created after the pre-processing\n",
    "    - window : this is the distance between two words algorithm should consider to find some relationship between them\n",
    "    \n",
    "    \n",
    "2. For glove() :\n",
    "    - no_of_components : This is the dimension of the output vector generated by the GloVe\n",
    "    - learning_rate : Algo uses gradient descent so learning rate defines the rate at which the algo reaches towards the minima (lower the rate more time it takes to learn but reaches the minimum value)\n",
    "\n",
    "\n",
    "3. For glove.fit() :\n",
    "    - cooccurence_matrix: the matrix of word-word co-occurrences\n",
    "    - epochs: this defines the number of passes algo makes through the data set\n",
    "    - no_of_threads: number of threads used by the algo to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n"
     ]
    }
   ],
   "source": [
    "#!pip install glove_python\n",
    "\n",
    "#importing the glove library\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# creating a corpus object\n",
    "corpus = Corpus() \n",
    "\n",
    "#training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "corpus.fit(tokens_list, window=3)\n",
    "#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
    "#We can set the learning rate as it uses Gradient Descent and number of components\n",
    "glove = Glove(no_components=300, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Displaying Glove WordVector of a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.word_vectors[glove.dictionary['people']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrain Glove\n",
    "\n",
    "Glove developers have also made available pre-computed embeddings for millions of English tokens, obtained from training Wikipedia data and Common crawl data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch pretrain glove word vectors \n",
    "import numpy as np \n",
    "pretrained_glove_embedding={}\n",
    "with open('/Users/kaustuv/DataScience/DS_tutorials/datasets/Glove_wordembeeding/glove.6B/glove.6B.300d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        pretrained_glove_embedding[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Displaying Glove pretrained WordVector of a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pretrained_glove_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_glove_embedding['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Fast-Text <a class=\"kk\" id=\"2.3\"></a>\n",
    "\n",
    "[FastText](https://fasttext.cc/) is a library for learning of word embeddings and text classification. The Facebook Research Team created fastText in Nov 2015. Fast-Text is an extension of word2vec library. It builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. FastText assumes a word to be formed by a n-grams of character for example, sunny is composed of [sun, sunn,sunny],[sunny,unny,nny]... etc, where n could range from 1 to the length of the word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. Thus even for previously unseen words, typo errors, and OOV (Out Of Vocabulary) words the model can make an educated guess towards its meaning.Obvious trade off is processing time. Gensim provides the [FastText implementation](https://radimrehurek.com/gensim/models/fasttext.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FastText Implementatation using gensim\n",
    "\n",
    "Refer : https://radimrehurek.com/gensim/models/fasttext.html for parameter deatails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension =300\n",
    "from gensim.models import FastText\n",
    "fasttext_model = FastText(tokens_list, size=dimension, window=5, min_count=5, workers=4, sg=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Displaying FastText WordVector of given word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model['people']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model.similarity('evacuation','shelter' )\n",
    "\n",
    "fasttext_model.most_similar('earthquake')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PreTrained Fasttext\n",
    "\n",
    "FastText developers have also made available pre-computed embeddings for millions of english tokens, obtained from training Wikipedia data and common crawl data. \n",
    "\n",
    "Disclaimer: Loading the fastText pretrain will consume some serious memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "EMBEDDING_FILE = '/Users/kaustuv/DataScience/DS_tutorials/datasets/FastText/wiki-news-300d-1M-subword.vec'\n",
    "pretrained_fasttext_embedding = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in (open(EMBEDDING_FILE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Displaying FastText pretrained WordVector of a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_fasttext_embedding['earthquake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(pretrained_fasttext_embedding['earthquake'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text to Numeric Convertion Using Word Vectors <a class=\"kk\" id=\"3\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "So we have learned about word embedding techniques and created word vectors for our corpus.  Now we will convert our textual data into numerical using these word vectors.  I will explain about two popular texts to numerical conversion techniques using word vectors, \n",
    "1. Vector Averaging  \n",
    "2. Embedding Matrix and Keras Embedding layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Vector Averaging  <a class=\"kk\" id=\"3.1\"></a>\n",
    "In this approach we directly averages all word embedding occurred in the text. Final length remains equal to word vector dimension. This is go to technique when we are planning to use standard machine learning models such a logistic regression, naïve-bayes, svm etc.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Averaging With Word2Vec <a class=\"kk\" id=\"3.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for Vector Averaging with word2Vec\n",
    "import numpy as np\n",
    "def w2v_embeddings(text,w2v_model,dimension):\n",
    "    if len(text) < 1:\n",
    "        return np.zeros(dimension)\n",
    "    else:\n",
    "        vectorized = [w2v_model[word] if word in w2v_model else np.random.rand(dimension) for word in text] \n",
    "    \n",
    "    sum = np.sum(vectorized,axis=0)\n",
    "    ## return the average\n",
    "    return sum/len(vectorized)     \n",
    "\n",
    "def get_w2v_embeddings(text,w2v_model,dimension):\n",
    "        embeddings = text.apply(lambda x: w2v_embeddings(x, w2v_model,dimension))\n",
    "        return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Text to numeric using Vector Averaging for sgmodel \n",
    "train_embeddings_sg_model  = get_w2v_embeddings(train_df['text'],sg_model,dimension=300)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_embeddings_sg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_embeddings_sg_model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Text to numeric using Vector Averaging for cbow model\n",
    "train_embeddings_cbow_model_  = get_w2v_embeddings(train_df['text'],cbow_model,dimension=300)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Averaging With Glove <a class=\"kk\" id=\"3.1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions  for Vector Averaging with GloVe\n",
    "import numpy as np\n",
    "def glove_embeddings(text, glove_model, dim ):\n",
    "    dic=glove_model.dictionary\n",
    "    if len(text) < 1:\n",
    "        return np.zeros(dim)\n",
    "    else:\n",
    "        vectorized = [glove_model.word_vectors[dic[word]] if word in dic else np.random.rand(dim) for word in text]  \n",
    "    sum = np.sum(vectorized,axis=0)\n",
    "    ## return the average\n",
    "    return sum/len(vectorized)     \n",
    "\n",
    "def get_glove_embeddings(text,glove_model,dimension):\n",
    "        embeddings = text.apply(lambda x: glove_embeddings(x,glove_model, dimension))\n",
    "        return list(embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to numeric using Averaging for glove\n",
    "import numpy as np\n",
    "train_embeddings_glove = get_glove_embeddings(train_df['text'],glove,dimension=300)\n",
    "test_embeddings_glove = get_glove_embeddings(test_df['text'],glove,dimension=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Averaging With Fasttext  <a class=\"kk\" id=\"3.1.3\"></a>\n",
    "\n",
    "As Fastext is an extension of word2vec hence the same averaging function of w2vec i.e.`get_ w2v_embeddings`  will work with fasttext too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "###  Text to numeric using Averaging with Fasttext\n",
    "import numpy as np\n",
    "fasttext_train_embeddings = w2v_embeddings(train_df['text'], fasttext_model,dimension=300)\n",
    "fasttext_test_embeddings = w2v_embeddings(test_df['text'],  fasttext_model,dimension=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Embedded Matrix & Keras Embedding layer <a class=\"kk\" id=\"3.2\"></a>\n",
    "\n",
    "Averaging is preferred choice when we intend to use ML models such as lr, svm, gbm etc. but our purpose here is to utilise Deep-learning algorithms. Deep Learning is a layer bases learning where each layer passes its learning to the next layer.   Few libraries have implement deep learning algorithms. A popular one among them is Keras. We will use Keras for our deep learning modelling purpose.\n",
    "\n",
    "For text processing Keras offers an embedding layer. This is the first layer of deep learning algorithm. Weights of the Embedding layer are of the shape (vocabulary_size, embedding_dimension) , this weight matrix is also called as Embedding matrix. We will first generate this embedding matrix from our word vectors and then initialize Keras embedding layer for each of our word embeddings. \n",
    "\n",
    "Moreover, Keras has built-in utilities for doing tokenization and encoding of text. We will use these utilities as they take care of a number of important features such as stripping special characters from strings, padding, fetching N most common words in dataset etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing using keras  tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer_obj=Tokenizer()\n",
    "# to builds the word index\n",
    "tokenizer_obj.fit_on_texts(tokens_list)\n",
    "# to turns strings into lists of integer indices.\n",
    "sequences=tokenizer_obj.texts_to_sequences(tokens_list)\n",
    "# defining maximum length of sequence \n",
    "MAX_LEN= 50\n",
    "# pad_sequences is used to ensure that all sequences in a list have the same length\n",
    "tweet_pad= pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "\n",
    "# segregating text & train from corpu\n",
    "x_train = tweet_pad[:7613]\n",
    "x_test = tweet_pad[7613:]\n",
    "\n",
    "targets =  [target for target in train_df['target']]\n",
    "\n",
    "# set of all word and their sequence no\n",
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))\n",
    "vocab_size = word_index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeeding_matrix(word_vector_model, dimension, vocab_size= vocab_size, word_index =word_index):\n",
    "    embedding_matrix=np.zeros((vocab_size,dimension))\n",
    "    for word,i in tqdm(word_index.items()):\n",
    "        if i > vocab_size:\n",
    "            continue\n",
    "        if word in word_vector_model:  \n",
    "            emb_vec=word_vector_model[word]\n",
    "            embedding_matrix[i]=emb_vec\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create keras embeeding layers for our word vectors.  we have created  4 trained word embedding model (skipgram, cbow, glove and fasttext) and 3 pretrain  model each for (word2vec ,glove and python) for all these seven we will crate keras word embedding model. \n",
    "\n",
    "### Word2Vec Embedding layers  <a class=\"kk\" id=\"3.2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trained skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28874 [00:00<?, ?it/s]/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n",
      "100%|██████████| 28874/28874 [00:00<00:00, 147284.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_matrix_sg_trained = generate_embeeding_matrix(sg_model, dimension = 300)\n",
    "\n",
    "embedding_layer_sg_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_sg_trained], \n",
    "                                     input_length=MAX_LEN, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-Trained  Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28874/28874 [00:00<00:00, 111852.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# pre trainde word2vec dimesion is 300\n",
    "embedding_matrix_w2v_pretrained = generate_embeeding_matrix(pretrained_w2vec_embedding, dimension =300)    \n",
    "\n",
    "embedding_layer_w2v_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_w2v_pretrained], \n",
    "                                     input_length=MAX_LEN, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trained  CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28874 [00:00<?, ?it/s]/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n",
      "100%|██████████| 28874/28874 [00:00<00:00, 164841.84it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_cbow_trained = generate_embeeding_matrix(cbow_model, dimension = 300)\n",
    "\n",
    "embedding_layer_cbow_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_cbow_trained], \n",
    "                                     input_length=MAX_LEN, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embedding Layers   <a class=\"kk\" id=\"3.2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trained Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28874/28874 [00:00<00:00, 467832.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embedding_matrix_glove_trained=np.zeros((vocab_size,300))\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > vocab_size:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=glove.word_vectors[glove.dictionary[word]]\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix_glove_trained[i]=emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_glove_trained = Embedding(vocab_size, dimension, weights=[embedding_matrix_glove_trained], \n",
    "                                     input_length=MAX_LEN, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PreTrained Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28874/28874 [00:00<00:00, 373207.89it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_glove_pretrained = generate_embeeding_matrix(pretrained_glove_embedding, dimension =300)    \n",
    "\n",
    "embedding_layer_glove_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_w2v_pretrained], \n",
    "                                     input_length=MAX_LEN, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext  Embedding layers  <a class=\"kk\" id=\"3.2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trained FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28874 [00:00<?, ?it/s]/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/Users/kaustuv/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n",
      "100%|██████████| 28874/28874 [00:02<00:00, 13341.40it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_fasttext_trained = generate_embeeding_matrix(fasttext_model, dimension =300)    \n",
    "\n",
    "embedding_layer_fasttext_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_fasttext_trained], \n",
    "                                     input_length=MAX_LEN, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-Trained FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28874/28874 [00:00<00:00, 362862.16it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_fasttext_pretrained = generate_embeeding_matrix(pretrained_fasttext_embedding, dimension =300)    \n",
    "\n",
    "embedding_layer_fasttext_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_fasttext_pretrained], \n",
    "                                     input_length=MAX_LEN, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Learning Models <a class=\"kk\" id=\"4\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    " We have initialized Keras embedding layer for our various word embedding models. Now it’s time to train using deep learning models. I will demonstrate how to train for glove pertained layer. You can test with other six embedding layer also (by just reassigning embedding_layer). One point you will notice that pretrained embedding layers performs much better than their trained counter parts. Again the purpose here is to depict basic Deep Learning model performance and not to obtain high score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare embeeding layer of your choics \n",
    "embedding_layer = embedding_layer_glove_pretrained\n",
    "\n",
    "# can try with other embedding layes too\n",
    "# embedding_layer_fasttext_pretrained\n",
    "# embedding_layer_fasttext_trained\n",
    "# embedding_layer_cbow_trained\n",
    "# embedding_layer_sg_trained\n",
    "# embedding_layer_w2vec_pretrained\n",
    "# embedding_layer_glove_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic DNN <a class=\"kk\" id=\"4.1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 50, 300)           8662500   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 15001     \n",
      "=================================================================\n",
      "Total params: 8,677,501\n",
      "Trainable params: 15,001\n",
      "Non-trainable params: 8,662,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/15\n",
      "6090/6090 [==============================] - 0s 69us/step - loss: 0.5592 - acc: 0.7322 - val_loss: 0.5083 - val_acc: 0.7735\n",
      "Epoch 2/15\n",
      "6090/6090 [==============================] - 0s 53us/step - loss: 0.4500 - acc: 0.8056 - val_loss: 0.4810 - val_acc: 0.7820\n",
      "Epoch 3/15\n",
      "6090/6090 [==============================] - 0s 52us/step - loss: 0.4049 - acc: 0.8327 - val_loss: 0.4698 - val_acc: 0.7945\n",
      "Epoch 4/15\n",
      "6090/6090 [==============================] - 0s 52us/step - loss: 0.3734 - acc: 0.8496 - val_loss: 0.4681 - val_acc: 0.7879\n",
      "Epoch 5/15\n",
      "6090/6090 [==============================] - 0s 57us/step - loss: 0.3480 - acc: 0.8619 - val_loss: 0.4670 - val_acc: 0.7853\n",
      "Epoch 6/15\n",
      "6090/6090 [==============================] - 0s 81us/step - loss: 0.3284 - acc: 0.8768 - val_loss: 0.4664 - val_acc: 0.7846\n",
      "Epoch 7/15\n",
      "6090/6090 [==============================] - 0s 63us/step - loss: 0.3112 - acc: 0.8849 - val_loss: 0.4696 - val_acc: 0.7905\n",
      "Epoch 8/15\n",
      "6090/6090 [==============================] - 0s 78us/step - loss: 0.2966 - acc: 0.8915 - val_loss: 0.4723 - val_acc: 0.7846\n",
      "Epoch 9/15\n",
      "6090/6090 [==============================] - 0s 68us/step - loss: 0.2833 - acc: 0.9002 - val_loss: 0.4760 - val_acc: 0.7859\n",
      "Epoch 10/15\n",
      "6090/6090 [==============================] - 0s 54us/step - loss: 0.2711 - acc: 0.9071 - val_loss: 0.4796 - val_acc: 0.7853\n",
      "Epoch 11/15\n",
      "6090/6090 [==============================] - 0s 65us/step - loss: 0.2606 - acc: 0.9154 - val_loss: 0.4840 - val_acc: 0.7853\n",
      "Epoch 12/15\n",
      "6090/6090 [==============================] - 0s 56us/step - loss: 0.2511 - acc: 0.9177 - val_loss: 0.4899 - val_acc: 0.7840\n",
      "Epoch 13/15\n",
      "6090/6090 [==============================] - 0s 56us/step - loss: 0.2419 - acc: 0.9213 - val_loss: 0.4931 - val_acc: 0.7827\n",
      "Epoch 14/15\n",
      "6090/6090 [==============================] - 0s 53us/step - loss: 0.2338 - acc: 0.9250 - val_loss: 0.5007 - val_acc: 0.7840\n",
      "Epoch 15/15\n",
      "6090/6090 [==============================] - 0s 53us/step - loss: 0.2258 - acc: 0.9281 - val_loss: 0.5069 - val_acc: 0.7840\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "dnn_model = Sequential()\n",
    "dnn_model.add(embedding_layer)\n",
    "dnn_model.add(Flatten())\n",
    "dnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "dnn_model.summary()\n",
    "\n",
    "history = dnn_model.fit(x_train,  y = targets,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.2 CNN <a class=\"kk\" id=\"4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 50, 300)           8662500   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 48, 3)             2703      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 21        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 8,665,304\n",
      "Trainable params: 2,804\n",
      "Non-trainable params: 8,662,500\n",
      "_________________________________________________________________\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/15\n",
      "6090/6090 [==============================] - 0s 48us/step - loss: 0.2191 - acc: 0.9328 - val_loss: 0.5096 - val_acc: 0.7768\n",
      "Epoch 2/15\n",
      "6090/6090 [==============================] - 0s 51us/step - loss: 0.2122 - acc: 0.9389 - val_loss: 0.5161 - val_acc: 0.7754\n",
      "Epoch 3/15\n",
      "6090/6090 [==============================] - 0s 52us/step - loss: 0.2058 - acc: 0.9394 - val_loss: 0.5220 - val_acc: 0.7807\n",
      "Epoch 4/15\n",
      "6090/6090 [==============================] - 0s 49us/step - loss: 0.2002 - acc: 0.9411 - val_loss: 0.5273 - val_acc: 0.7814\n",
      "Epoch 5/15\n",
      "6090/6090 [==============================] - 0s 48us/step - loss: 0.1943 - acc: 0.9435 - val_loss: 0.5348 - val_acc: 0.7794\n",
      "Epoch 6/15\n",
      "6090/6090 [==============================] - 0s 51us/step - loss: 0.1889 - acc: 0.9470 - val_loss: 0.5409 - val_acc: 0.7761\n",
      "Epoch 7/15\n",
      "6090/6090 [==============================] - 0s 52us/step - loss: 0.1838 - acc: 0.9494 - val_loss: 0.5427 - val_acc: 0.7794\n",
      "Epoch 8/15\n",
      "6090/6090 [==============================] - 0s 51us/step - loss: 0.1795 - acc: 0.9514 - val_loss: 0.5481 - val_acc: 0.7787\n",
      "Epoch 9/15\n",
      "6090/6090 [==============================] - 0s 51us/step - loss: 0.1748 - acc: 0.9525 - val_loss: 0.5526 - val_acc: 0.7754\n",
      "Epoch 10/15\n",
      "6090/6090 [==============================] - 0s 50us/step - loss: 0.1706 - acc: 0.9555 - val_loss: 0.5587 - val_acc: 0.7794\n",
      "Epoch 11/15\n",
      "6090/6090 [==============================] - 0s 49us/step - loss: 0.1664 - acc: 0.9568 - val_loss: 0.5664 - val_acc: 0.7702\n",
      "Epoch 12/15\n",
      "6090/6090 [==============================] - 0s 48us/step - loss: 0.1625 - acc: 0.9616 - val_loss: 0.5740 - val_acc: 0.7715\n",
      "Epoch 13/15\n",
      "6090/6090 [==============================] - 0s 51us/step - loss: 0.1585 - acc: 0.9601 - val_loss: 0.5783 - val_acc: 0.7761\n",
      "Epoch 14/15\n",
      "6090/6090 [==============================] - 0s 52us/step - loss: 0.1550 - acc: 0.9606 - val_loss: 0.5874 - val_acc: 0.7748\n",
      "Epoch 15/15\n",
      "6090/6090 [==============================] - 0s 51us/step - loss: 0.1518 - acc: 0.9616 - val_loss: 0.5905 - val_acc: 0.7761\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "cnn_model = Sequential()\n",
    "# note here we are adding embeeding layer\n",
    "cnn_model.add(embedding_layer)\n",
    "cnn_model.add(keras.layers.Dropout(0.2))\n",
    "cnn_model.add(keras.layers.Conv1D(3,3, padding='valid',activation='relu', strides=1))\n",
    "cnn_model.add(keras.layers.GlobalMaxPooling1D())\n",
    "cnn_model.add(keras.layers.Dense(20))\n",
    "cnn_model.add(keras.layers.Dropout(0.2))\n",
    "cnn_model.add(keras.layers.Activation('relu'))\n",
    "cnn_model.add(keras.layers.Dense(1))\n",
    "cnn_model.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "# Get model summary\n",
    "cnn_model.summary()\n",
    "\n",
    "# compile the model\n",
    "history = cnn_model.fit(x_train,  y = targets,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Simple RNN <a class=\"kk\" id=\"4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/15\n",
      "6090/6090 [==============================] - 3s 437us/step - loss: 0.6008 - acc: 0.6961 - val_loss: 0.5491 - val_acc: 0.7564\n",
      "Epoch 2/15\n",
      "6090/6090 [==============================] - 2s 383us/step - loss: 0.5408 - acc: 0.7576 - val_loss: 0.5350 - val_acc: 0.7623\n",
      "Epoch 3/15\n",
      "6090/6090 [==============================] - 2s 375us/step - loss: 0.5044 - acc: 0.7741 - val_loss: 0.4989 - val_acc: 0.7892\n",
      "Epoch 4/15\n",
      "6090/6090 [==============================] - 2s 378us/step - loss: 0.4721 - acc: 0.7970 - val_loss: 0.4576 - val_acc: 0.7984\n",
      "Epoch 5/15\n",
      "6090/6090 [==============================] - 2s 377us/step - loss: 0.4601 - acc: 0.8003 - val_loss: 0.5171 - val_acc: 0.7511\n",
      "Epoch 6/15\n",
      "6090/6090 [==============================] - 2s 377us/step - loss: 0.4571 - acc: 0.8018 - val_loss: 0.4715 - val_acc: 0.7820\n",
      "Epoch 7/15\n",
      "6090/6090 [==============================] - 2s 380us/step - loss: 0.4461 - acc: 0.8131 - val_loss: 0.4430 - val_acc: 0.8096\n",
      "Epoch 8/15\n",
      "6090/6090 [==============================] - 2s 372us/step - loss: 0.4481 - acc: 0.8158 - val_loss: 0.4917 - val_acc: 0.7761\n",
      "Epoch 9/15\n",
      "6090/6090 [==============================] - 2s 397us/step - loss: 0.4453 - acc: 0.8179 - val_loss: 0.4844 - val_acc: 0.7997\n",
      "Epoch 10/15\n",
      "6090/6090 [==============================] - 2s 401us/step - loss: 0.4322 - acc: 0.8200 - val_loss: 0.4728 - val_acc: 0.8070\n",
      "Epoch 11/15\n",
      "6090/6090 [==============================] - 3s 434us/step - loss: 0.4256 - acc: 0.8273 - val_loss: 0.4755 - val_acc: 0.7938\n",
      "Epoch 12/15\n",
      "6090/6090 [==============================] - 3s 419us/step - loss: 0.4240 - acc: 0.8258 - val_loss: 0.4912 - val_acc: 0.7873\n",
      "Epoch 13/15\n",
      "6090/6090 [==============================] - 2s 384us/step - loss: 0.4152 - acc: 0.8327 - val_loss: 0.4630 - val_acc: 0.7991\n",
      "Epoch 14/15\n",
      "6090/6090 [==============================] - 2s 377us/step - loss: 0.4160 - acc: 0.8294 - val_loss: 0.4944 - val_acc: 0.7406\n",
      "Epoch 15/15\n",
      "6090/6090 [==============================] - 2s 377us/step - loss: 0.4142 - acc: 0.8328 - val_loss: 0.4928 - val_acc: 0.7997\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(embedding_layer)\n",
    "rnn_model.add(SimpleRNN(32))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = rnn_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  Recurrent Neural Network -LSTM <a class=\"kk\" id=\"4.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/10\n",
      "6090/6090 [==============================] - 5s 820us/step - loss: 0.5534 - acc: 0.7307 - val_loss: 0.4505 - val_acc: 0.8056\n",
      "Epoch 2/10\n",
      "6090/6090 [==============================] - 4s 729us/step - loss: 0.4525 - acc: 0.8082 - val_loss: 0.4556 - val_acc: 0.8227\n",
      "Epoch 3/10\n",
      "6090/6090 [==============================] - 4s 737us/step - loss: 0.4284 - acc: 0.8169 - val_loss: 0.4523 - val_acc: 0.7938\n",
      "Epoch 4/10\n",
      "6090/6090 [==============================] - 5s 739us/step - loss: 0.4204 - acc: 0.8264 - val_loss: 0.5697 - val_acc: 0.7905\n",
      "Epoch 5/10\n",
      "6090/6090 [==============================] - 5s 747us/step - loss: 0.4149 - acc: 0.8268 - val_loss: 0.4468 - val_acc: 0.8011\n",
      "Epoch 6/10\n",
      "6090/6090 [==============================] - 5s 760us/step - loss: 0.3991 - acc: 0.8358 - val_loss: 0.4576 - val_acc: 0.8063\n",
      "Epoch 7/10\n",
      "6090/6090 [==============================] - 5s 779us/step - loss: 0.3947 - acc: 0.8378 - val_loss: 0.4218 - val_acc: 0.8037\n",
      "Epoch 8/10\n",
      "6090/6090 [==============================] - 6s 986us/step - loss: 0.3811 - acc: 0.8466 - val_loss: 0.4417 - val_acc: 0.8070\n",
      "Epoch 9/10\n",
      "6090/6090 [==============================] - 5s 788us/step - loss: 0.3737 - acc: 0.8473 - val_loss: 0.4549 - val_acc: 0.8017\n",
      "Epoch 10/10\n",
      "6090/6090 [==============================] - 4s 716us/step - loss: 0.3653 - acc: 0.8534 - val_loss: 0.4346 - val_acc: 0.8004\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(32))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = lstm_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Recurrent Neural Network – GRU <a class=\"kk\" id=\"4.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/22\n",
      "6090/6090 [==============================] - 6s 955us/step - loss: 0.6816 - acc: 0.5793 - val_loss: 0.6837 - val_acc: 0.5345\n",
      "Epoch 2/22\n",
      "6090/6090 [==============================] - 5s 898us/step - loss: 0.6049 - acc: 0.6805 - val_loss: 0.5685 - val_acc: 0.7459\n",
      "Epoch 3/22\n",
      "6090/6090 [==============================] - 5s 883us/step - loss: 0.5047 - acc: 0.7778 - val_loss: 0.4759 - val_acc: 0.7965\n",
      "Epoch 4/22\n",
      "6090/6090 [==============================] - 5s 887us/step - loss: 0.4584 - acc: 0.8136 - val_loss: 0.4626 - val_acc: 0.8024\n",
      "Epoch 5/22\n",
      "6090/6090 [==============================] - 5s 902us/step - loss: 0.4308 - acc: 0.8250 - val_loss: 0.5075 - val_acc: 0.7938\n",
      "Epoch 6/22\n",
      "6090/6090 [==============================] - 5s 893us/step - loss: 0.4132 - acc: 0.8323 - val_loss: 0.4249 - val_acc: 0.8142\n",
      "Epoch 7/22\n",
      "6090/6090 [==============================] - 6s 1ms/step - loss: 0.3987 - acc: 0.8402 - val_loss: 0.4742 - val_acc: 0.8116\n",
      "Epoch 8/22\n",
      "6090/6090 [==============================] - 6s 1ms/step - loss: 0.3929 - acc: 0.8420 - val_loss: 0.4372 - val_acc: 0.8096\n",
      "Epoch 9/22\n",
      "6090/6090 [==============================] - 6s 979us/step - loss: 0.3842 - acc: 0.8511 - val_loss: 0.4405 - val_acc: 0.8155\n",
      "Epoch 10/22\n",
      "6090/6090 [==============================] - 6s 1ms/step - loss: 0.3768 - acc: 0.8548 - val_loss: 0.4890 - val_acc: 0.7807\n",
      "Epoch 11/22\n",
      "6090/6090 [==============================] - 6s 976us/step - loss: 0.3704 - acc: 0.8583 - val_loss: 0.4237 - val_acc: 0.8122\n",
      "Epoch 12/22\n",
      "6090/6090 [==============================] - 6s 924us/step - loss: 0.3617 - acc: 0.8617 - val_loss: 0.4431 - val_acc: 0.8089\n",
      "Epoch 13/22\n",
      "6090/6090 [==============================] - 6s 912us/step - loss: 0.3542 - acc: 0.8680 - val_loss: 0.4291 - val_acc: 0.8155\n",
      "Epoch 14/22\n",
      "6090/6090 [==============================] - 5s 879us/step - loss: 0.3416 - acc: 0.8721 - val_loss: 0.5209 - val_acc: 0.7879\n",
      "Epoch 15/22\n",
      "6090/6090 [==============================] - 5s 858us/step - loss: 0.3349 - acc: 0.8791 - val_loss: 0.4903 - val_acc: 0.7945\n",
      "Epoch 16/22\n",
      "6090/6090 [==============================] - 5s 883us/step - loss: 0.3305 - acc: 0.8782 - val_loss: 0.5104 - val_acc: 0.7754\n",
      "Epoch 17/22\n",
      "6090/6090 [==============================] - 5s 869us/step - loss: 0.3195 - acc: 0.8869 - val_loss: 0.5098 - val_acc: 0.7965\n",
      "Epoch 18/22\n",
      "6090/6090 [==============================] - 6s 906us/step - loss: 0.3136 - acc: 0.8887 - val_loss: 0.5025 - val_acc: 0.7958\n",
      "Epoch 19/22\n",
      "6090/6090 [==============================] - 5s 815us/step - loss: 0.3047 - acc: 0.8939 - val_loss: 0.5190 - val_acc: 0.7958\n",
      "Epoch 20/22\n",
      "6090/6090 [==============================] - 5s 868us/step - loss: 0.2978 - acc: 0.8972 - val_loss: 0.5081 - val_acc: 0.8070\n",
      "Epoch 21/22\n",
      "6090/6090 [==============================] - 5s 857us/step - loss: 0.2895 - acc: 0.9028 - val_loss: 0.4827 - val_acc: 0.8004\n",
      "Epoch 22/22\n",
      "6090/6090 [==============================] - 5s 849us/step - loss: 0.2841 - acc: 0.9007 - val_loss: 0.5298 - val_acc: 0.7971\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "gru_model = Sequential()\n",
    "gru_model.add(embedding_layer)\n",
    "gru_model.add(GRU(32))\n",
    "gru_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "gru_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = gru_model.fit(x_train, y = targets,epochs=22, batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Target Prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_preds = model.predict(x_test)\n",
    "preds = raw_preds.round().astype(int)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading! Purpose of this notebook is to provide fair understanding of word embedding techniques and get beginners started in quick time. Do give your feed back. In the next part-3 we will read about state-of-art 'BERT Embedding'.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1081639/too%20much.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587151146&Signature=revsFWYk5EZkV%2B26mNr6q8vzpAkvHsiA8TrtphPck5DU%2FxcT9iSbaDlUkFwTtXmoLPTkVLwylkc9cbswCWJTHBogYaNUF%2Bv3YXdwHGurX1lbJ6SH41ZR%2B9%2BwpMmIIBv%2FMYenQUdO5ERJbfbMW%2BFK6rxFbJizkUwAQhy4eDpbywlhLu1l2P79JUsHr5uf6L8fAOzaf4CjQTC1VP5TjG%2BERvLzwnBQO1oN9g9%2B2Y%2FeL0LXHNL17297xfX8pZ3j%2FGt2hl%2BXWYP3nW5ymkuM4rpa5TbkQDac0qELQrZy8ncHbDqtwVj9csjFK3J0ALwmRLUryuLqdvh3v6cnS1sd3kU%2FBQ%3D%3D\" width=\"250\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Deep Learning with Python by FRANÇOIS CHOLLET  http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf\n",
    "- https://en.wikipedia.org/\n",
    "- https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/eb9cd609-e44a-40a2-9c3a-f16fc4f5289a.xhtml\n",
    "- https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html\n",
    "- https://www.kaggle.com/slatawa/simple-implementation-of-word2vec\n",
    "- https://becominghuman.ai/how-does-word2vecs-skip-gram-work-f92e0525def4 (image)\n",
    "- https://www.thinkinfi.com/2019/06/single-word-cbow.html(image)\n",
    "- https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer\n",
    "- https://medium.com/@japneet121/word-vectorization-using-glove-76919685ee0b\n",
    "- https://www.kaggle.com/christofhenkel/fasttext-starter-description-only\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
