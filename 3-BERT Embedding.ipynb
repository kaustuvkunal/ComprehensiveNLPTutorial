{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is part 3 of 'Comprehensive NLP Tutorial' series'. In  [Part-1](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-1-ml-perspective) we performed text processing using ML techniques and in [Part-2](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-2-dl-perspective) we used Word Embedding and Deep Learning algorithms. In this part mainly we will look into state-of-art 'BERT' technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"kk\" id=\"0.1\"></a>\n",
    "## Contents\n",
    "\n",
    "1. [BERT Introduction](#1)\n",
    "1. [Salient BERT Features](#2)\n",
    "1. [BERT Implementation](#3)\n",
    "    1. [BERT Pretrained Layer](#3.1)\n",
    "    1. [BERT Encoding](#3.2)\n",
    "        1. [Encoding Sample Example](#3.2.1)\n",
    "        1. [Encoding The Dataset](#3.2.2)\n",
    "    1. [BERT Modeling](#3.3)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BERT  Introduction  <a class=\"kk\" id=\"1\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "[BERT](https://github.com/google-research/bert) stands for <B> Bidirectional Encoder Representations from Transformers </B>. It was created and published in 2018 by Jacob Devlin and his colleagues from Google. BERT [paper](https://arxiv.org/pdf/1810.04805.pdf) depicts that a directionally trained language model can have a deeper sense of language context and flow than single-direction language models. Since creation BERT is a go to model for NLP and has inspired many NLP architecture such as RoBERTa, XLNet etc.\n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/598303/1088431/bert.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1587371932&Signature=L%2FPKC4s3HSp8wLRL2n6%2BgBEpPewTYSJlaBXkO3CyJmMfv%2BlIpYNq7YmK0W57RxmlCkbWcScAU%2BIGUMDO7dwKAwL0s%2F7345AA4suyA3BbQ61bLDJx5HKOz27lnNfQBPP1dpZQ4brnKDxAzcCsS8hOFArf0iZS1%2BHrTHCdWWm2%2Bah9LKmWy8%2F3NDgUJazdZAw76LSP4ULeG4IYNUbkkET%2F4e6zDQw2YGVeQhBamnpmQ3p8FjfMzRNNlyZZfWKBIdtNjl3iomyImAdq8gt0UgTOuHT5%2F2CoZsMLOjM%2B5AIerEf45KZ3FZl5j285%2FqD6pq3IcN4KLvTxyshIrpWYSK2u8Q%3D%3D\" width=\"250\">\n",
    "\n",
    "\n",
    "Lets understand BERT by breaking BERT abbreviation,\n",
    "\n",
    "- <B>Bidirectional</B> :  BERT takes whole text passage as input and reads passage in both direction to understand the meaning of each word.\n",
    "\n",
    "\n",
    "- <B>Transformers</B> :  BERT is based on a Deep Transformer network. Transformer network is a type of network that can process efficiently long texts by using attention. An attention is a mechanism to learn contextual relations between words (or sub-words) in a text. \n",
    "\n",
    "\n",
    "- <B>Encoder Representation</B> : Originally  Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task, since BERT’s goal is to generate a language model only the encoder mechanism is necessary hence 'encoder representation'\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> How BERT performs Bidirectional training? </B>\n",
    "\n",
    "BERT uses following two prediction models simultaneously with the goal of minimizing the combined loss function of the two strategies\n",
    "\n",
    "1. <B>Masked Language Model </B>:  Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence.\n",
    "\n",
    "\n",
    "2. <B>Next Sentence Prediction </B> :  The model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Salient BERT Features  <a class=\"kk\" id=\"2\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "\n",
    "1. <B>Dynamic Word Embedding </B>:Unlike previous [Part-2](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-2-dl-perspective) word embedding techniques (Word2Vec etc), where each word had a static vector representation, BERT produces word representations that are not fixed and dynamically informed by the words around them.   \n",
    "2. <B>Pre trained Model</B> : BERT uses transfer learning i.e. it fetches knowledge of pretrained model and then fine-tune it. BERT is pre-trained on a large corpus of unlabelled text including the entire Wikipedia and Book Corpus. The pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models. BERT is clearly not designed to train any dataset from scratch.\n",
    "3. <B>Word Segmentation </B>:BERT does not flag OOV and unknown word like 'OOV' and 'UNK' (as   in fasttext). It decomposes these into meaningful sub-word and characters tokens and then generates embeddings.\n",
    "\n",
    "4. <B> Multilanguage Support </B>: BERT had been adopted by Google Search for over 70 languages.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  BERT  Implementation  <a class=\"kk\" id=\"3\"></a>\n",
    "[Back to Contents](#0.1)\n",
    "\n",
    "We will use Goolge TensorFlow [library](https://pypi.org/project/bert-for-tf2/) for our BERT modeling. It uses Keras backend.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 BERT Pretrained Layer  <a class=\"kk\" id=\"3.1\"></a>\n",
    "\n",
    "First we will fetch our pretrained BERT layer and load the tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  to install tesorflow bert package\n",
    "#!pip install bert-for-tf2\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "\n",
    "#Loding pretrained bert layer\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "# Loading tokenizer from the bert layer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocab_file, do_lower_case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 BERT  Encoding  <a class=\"kk\" id=\"3.2\"></a>\n",
    "\n",
    "\n",
    "-  Each sentence is first tokenized into tokens \n",
    "-  A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "-  Tokens that comply with the fixed vocabulary are fetched and assigned with following 3 properties.\n",
    "    1. Token IDs -  A Unique token-id from BERT’s tokenizer.\n",
    "    2. Padding ID (Mask-Id) - to indicate which elements in the sequence are tokens and which are padding elements.\n",
    "    3. Segment IDs - to distinguish different sentences.   \n",
    "\n",
    "Lets encode a sample text \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 BERT Text Encoding : Sample Example  <a class=\"kk\" id=\"3.2.1\"></a>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after tokenization\n",
      "['it', 'may', 'have', 'worked', 'better', 'with', 'more', 'examples']\n",
      "After adding [CLS] and [SEP]: \n",
      "['[CLS]', 'it', 'may', 'have', 'worked', 'better', 'with', 'more', 'examples', '[SEP]']\n",
      "tokens to id \n",
      "[101, 2009, 2089, 2031, 2499, 2488, 2007, 2062, 4973, 102]\n",
      "tokens: \n",
      "[101, 2009, 2089, 2031, 2499, 2488, 2007, 2062, 4973, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "15\n",
      "Pad Masking: \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segment Ids: \n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text = 'It may have worked better with more examples'\n",
    "# tokenize\n",
    "tokens_list = tokenizer.tokenize(text)\n",
    "print('Text after tokenization')\n",
    "print(tokens_list)\n",
    "\n",
    "# initilize dimension\n",
    "max_len =25\n",
    "text = tokens_list[:max_len-2]\n",
    "input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "print(\"After adding [CLS] and [SEP]: \")\n",
    "print(input_sequence)\n",
    "\n",
    "\n",
    "tokens = tokenizer.convert_tokens_to_ids(input_sequence )\n",
    "print(\"tokens to id \")\n",
    "print(tokens)\n",
    "\n",
    "pad_len = max_len -len(input_sequence)\n",
    "tokens += [0] * pad_len\n",
    "print(\"tokens: \")\n",
    "print(tokens)\n",
    "\n",
    "print(pad_len)\n",
    "pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "print(\"Pad Masking: \")\n",
    "print(pad_masks)\n",
    "\n",
    "segment_ids = [0] * max_len\n",
    "print(\"Segment Ids: \")\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Encoding Dataset  <a class=\"kk\" id=\"3.2.2\"></a>\n",
    " \n",
    "Now lets  fetch and encode our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch & cleaning  datsset\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk \n",
    "import re\n",
    "\n",
    "train_df = pd.read_csv(\"nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"nlp-getting-started/test.csv\")\n",
    "\n",
    "\n",
    "def convert_to_antonym(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = []\n",
    "    temp_word = ''\n",
    "    for word in words:\n",
    "        antonyms = []\n",
    "        if word == 'not':\n",
    "            temp_word = 'not_'\n",
    "        elif temp_word == 'not_':\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for s in syn.lemmas():\n",
    "                    for a in s.antonyms():\n",
    "                        antonyms.append(a.name())\n",
    "            if len(antonyms) >= 1:\n",
    "                word = antonyms[0]\n",
    "            else:\n",
    "                word = temp_word + word # when antonym is not found, it will\n",
    "                                    # remain not_happy\n",
    "            \n",
    "            temp_word = ''\n",
    "        if word != 'not':\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def correct_spellings(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_words.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "        \n",
    "\n",
    " \n",
    " \n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "  \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text= re.sub(r'[^\\w\\s#]',' ',text) #Removing every thing other than space, word and hash\n",
    "    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n",
    "    text= re.sub(r'[0-9]',' ',text)\n",
    "    #text = correct_spellings(text)\n",
    "    text = convert_to_antonym(text)\n",
    "    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    \n",
    "    return text\n",
    "\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "sentences= pd.DataFrame(columns=['text'])\n",
    "sentences['text']= pd.concat([train_df[\"text\"], test_df[\"text\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to encode the text into tokens, masks, and segment flags\n",
    "import numpy as np\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "MAX_LEN = 64\n",
    "\n",
    "# encode train set \n",
    "train_input = bert_encode(train_df.text.values, tokenizer, max_len=MAX_LEN)\n",
    "# encode  test set \n",
    "test_input = bert_encode(test_df.text.values, tokenizer, max_len= MAX_LEN )\n",
    "train_labels = train_df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  101,  2256, 15616, ...,     0,     0,     0],\n",
       "        [  101,  3224,  2543, ...,     0,     0,     0],\n",
       "        [  101,  2035,  3901, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1049, 11396, ...,     0,     0,     0],\n",
       "        [  101,  2610, 11538, ...,     0,     0,     0],\n",
       "        [  101,  1996,  6745, ...,     0,     0,     0]]),\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]]),\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see encoded train set \n",
    "train_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input has 3 array first for token id , second for mask id and third for segment id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 BERT Modeling  <a class=\"kk\" id=\"3.3\"></a>\n",
    "\n",
    "Lets build & train a basic BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 768)]        0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            769         tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 769\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/2\n",
      "6090/6090 [==============================] - 681s 112ms/sample - loss: 0.6920 - accuracy: 0.5793 - val_loss: 0.7213 - val_accuracy: 0.5351\n",
      "Epoch 2/2\n",
      "6090/6090 [==============================] - 754s 124ms/sample - loss: 0.6880 - accuracy: 0.5790 - val_loss: 0.7155 - val_accuracy: 0.5351\n"
     ]
    }
   ],
   "source": [
    "# first define input for token, mask and segment id  \n",
    "from tensorflow.keras.layers import  Input\n",
    "input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "#  output  \n",
    "from tensorflow.keras.layers import Dense\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])  \n",
    "clf_output = sequence_output[:, 0, :]\n",
    "out = Dense(1, activation='sigmoid')(clf_output)   \n",
    "\n",
    "# intilize model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# train\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=2,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = test_pred.round().astype(int)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "- https://peltarion.com/knowledge-center/tutorials/bert-movie-review-sentiment-analysis\n",
    "- https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/\n",
    "- https://github.com/imgarylai/bert-embedding\n",
    "- http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "- http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "- https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "- https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "- https://androidkt.com/simple-text-classification-using-bert-in-tensorflow-keras-2-0/\n",
    "- https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "- https://www.kaggle.com/ratan123/in-depth-guide-to-google-s-bert\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
